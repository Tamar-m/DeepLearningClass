{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tamar-m/DeepLearningClass/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code adapted from: https://github.com/ahmetumutdurmus/zaremba/blob/master/main.py\n",
        "\n",
        "See also relevant paper: https://arxiv.org/pdf/1409.2329.pdf)"
      ],
      "metadata": {
        "id": "OpRQ96iMLemu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y6jzjADXuExQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76cccd01-5742-4cf6-c87c-c54d297261b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at Google_drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('Google_drive')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data must be google drive in following path: /Deep Learning Class/Ex2Data\n",
        "def data_load():\n",
        "    with open(\"/content/Google_drive/MyDrive/Deep Learning Class/Ex2Data/ptb.train.txt\") as f:\n",
        "        file = f.read()\n",
        "        trn = file[1:].split(' ')\n",
        "    with open(\"/content/Google_drive/MyDrive/Deep Learning Class/Ex2Data/ptb.valid.txt\") as f:\n",
        "        file = f.read()\n",
        "        vld = file[1:].split(' ')\n",
        "    with open(\"/content/Google_drive/MyDrive/Deep Learning Class/Ex2Data/ptb.test.txt\") as f:\n",
        "        file = f.read()\n",
        "        tst = file[1:].split(' ')\n",
        "    words = sorted(set(trn))\n",
        "    char2ind = {c: i for i, c in enumerate(words)}\n",
        "    trn = [char2ind[c] for c in trn]\n",
        "    vld = [char2ind[c] for c in vld]\n",
        "    tst = [char2ind[c] for c in tst]\n",
        "    return np.array(trn).reshape(-1, 1), np.array(vld).reshape(-1, 1), np.array(tst).reshape(-1, 1), len(words)\n",
        "\n",
        "#create minibatches of the data\n",
        "def minibatch(data, batch_size, seq_length):\n",
        "    data = torch.tensor(data, dtype = torch.int64)\n",
        "    num_batches = data.size(0)//batch_size\n",
        "    data = data[:num_batches*batch_size]\n",
        "    data=data.view(batch_size,-1)\n",
        "    dataset = []\n",
        "    for i in range(0,data.size(1)-1,seq_length):\n",
        "        seqlen=int(np.min([seq_length,data.size(1)-1-i]))\n",
        "        if seqlen<data.size(1)-1-i:\n",
        "            x=data[:,i:i+seqlen].transpose(1, 0)\n",
        "            y=data[:,i+1:i+seqlen+1].transpose(1, 0)\n",
        "            dataset.append((x, y))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "ezJmXB7IuHcD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding module\n",
        "class Embed(nn.Module): #embedding is efficient when we have a large number of input features\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.W = nn.Parameter(torch.Tensor(vocab_size, embed_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.W[x]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Embedding(vocab: {}, embedding: {})\".format(self.vocab_size, self.embed_size)\n",
        "\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "        self.b = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = torch.addmm(self.b, x.view(-1, x.size(2)), self.W.t())\n",
        "        return z\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"FC(input: {}, output: {})\".format(self.input_size, self.hidden_size)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, layer_num, dropout, winit, base):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layer_num = layer_num\n",
        "        self.winit = winit\n",
        "        self.embed = Embed(vocab_size, hidden_size)\n",
        "        if base == 'gru':\n",
        "          self.rnns = [nn.GRU(hidden_size, hidden_size) for i in range(layer_num)] #two GRU layers with 200 units per layer\n",
        "        else:\n",
        "          self.rnns = [nn.LSTM(hidden_size, hidden_size) for i in range(layer_num)] #two LSTM layers with 200 units per layer\n",
        "        self.rnns = nn.ModuleList(self.rnns)\n",
        "        self.fc = Linear(hidden_size, vocab_size) \n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.reset_parameters() #weights are reset uniformly when initializing the model\n",
        "        \n",
        "    def reset_parameters(self): #LSTM hidden layers weights initialized uniformly between -0.1 to 0.1\n",
        "        for param in self.parameters():\n",
        "            nn.init.uniform_(param, -self.winit, self.winit) \n",
        "            \n",
        "    def state_init(self, batch_size,base): #hidden states are initialized to zero, input of gru is h_0 and input of lstm is (h_0, c_0) \n",
        "        dev = next(self.parameters()).device\n",
        "        if base == 'gru':\n",
        "          states = [(torch.zeros(1, batch_size, layer.hidden_size, device = dev)) for layer in self.rnns]\n",
        "        else:\n",
        "          states = [(torch.zeros(1, batch_size, layer.hidden_size, device = dev), torch.zeros(1, batch_size, layer.hidden_size, device = dev)) for layer in self.rnns]\n",
        "        return states \n",
        "\n",
        "    # detach to complete truncated backpropagation through time (BPTT)\n",
        "    def detach(self, states, base):\n",
        "      if base == 'lstm':\n",
        "        return [(h.detach(), c.detach()) for (h,c) in states] # returns tensors that don't require a gradient\n",
        "      else:\n",
        "        return [h.detach() for h in states] #gru layers input of shape h_0\n",
        "\n",
        "    def forward(self, x, states):\n",
        "        x = self.embed(x)\n",
        "        x = self.dropout(x)\n",
        "        for i, rnn in enumerate(self.rnns):\n",
        "            x, states[i] = rnn(x, states[i])\n",
        "            x = self.dropout(x)\n",
        "        scores = self.fc(x)\n",
        "        return scores, states"
      ],
      "metadata": {
        "id": "w6j8oL5x5mbg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we define the loss (negative log likelihood) and perplexity"
      ],
      "metadata": {
        "id": "eLcQ38-oPJyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nll_loss(scores, y):\n",
        "    batch_size = y.size(1)\n",
        "    expscores = scores.exp()\n",
        "    probabilities = expscores / expscores.sum(1, keepdim = True)\n",
        "    answerprobs = probabilities[range(len(y.reshape(-1))), y.reshape(-1)]\n",
        "    return torch.mean(-torch.log(answerprobs) * batch_size)\n",
        "\n",
        "\n",
        "def perplexity(data,model,base):\n",
        "  with torch.no_grad():\n",
        "    losses = []\n",
        "    states = model.state_init(20,base)\n",
        "    for x,y in data:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      scores, states = model(x,states)\n",
        "      loss = nll_loss(scores, y)\n",
        "      losses.append(loss.data.item()/20)\n",
        "    return np.exp(np.mean(losses))"
      ],
      "metadata": {
        "id": "wxbdG19pYu8u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(data, model, lr, max_norm,base):\n",
        "    trn, vld, tst = data\n",
        "    total_words = 0\n",
        "    states = model.state_init(20,base) # batch size of 20 according to article\n",
        "    model.train()\n",
        "    for i, (x, y) in enumerate(trn):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        total_words += x.numel()\n",
        "        model.zero_grad()\n",
        "        states = model.detach(states, 'lstm')\n",
        "        scores, states = model(x, states)\n",
        "        loss = nll_loss(scores, y)\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "            for param in model.parameters():\n",
        "                param -= lr * param.grad\n",
        "        if i % (len(trn)//10) == 0:\n",
        "            print(\"batch no = {:d} / {:d}, \".format(i, len(trn)) +\n",
        "                  \"train loss = {:.3f}, \".format(loss.item()/20) + #calculate loss\n",
        "                  \"lr = {:.3f}, \".format(lr))\n",
        "    model.eval()\n",
        "    val_perp = perplexity(vld, model,base)\n",
        "    print(\"Validation set perplexity : {:.3f}\".format(val_perp))\n",
        "    print(\"*************************************************\\n\")\n",
        "    return val_perp"
      ],
      "metadata": {
        "id": "i5Mze4n--UyX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(data,epochs,epoch_threshold,lr,model,model_name,base):\n",
        "  train, validation, test, word_len = data_load()\n",
        "  train = minibatch(train,20,20)\n",
        "  validation = minibatch(validation, 20, 20)\n",
        "  test = minibatch(test,20,20)\n",
        "  output_data = {\"model\": model_name,\n",
        "                 \"train_perp\": [],\n",
        "                 \"test_perp\":[],\n",
        "                 \"val_perp\":[]\n",
        "                 }\n",
        "  best_perp = np.Inf\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      if epoch > epoch_threshold: \n",
        "        lr = lr / 2 # after 4 epochs, learning rate decreased by a factor of two for each epoch\n",
        "      print(f'Epoch {epoch}/{epochs}')\n",
        "      val_perp = train_one_epoch((train,validation,test),model,lr,5,base)   \n",
        "      model.eval()   \n",
        "      test_perp = perplexity(test, model,base)\n",
        "      train_perp = perplexity(train, model,base)\n",
        "      output_data[\"train_perp\"].append(train_perp)\n",
        "      output_data[\"test_perp\"].append(test_perp)\n",
        "      output_data[\"val_perp\"].append(test_perp)\n",
        "      filename = model_name + '.json'\n",
        "      if test_perp < best_perp:\n",
        "        state = {\n",
        "                'model': model.state_dict(),\n",
        "                'train_perp': train_perp,\n",
        "                'test_perp': test_perp,\n",
        "                'epoch': epoch,\n",
        "                }\n",
        "        best_perp = test_perp\n",
        "        torch.save(state, model_name + '.pt')\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "          json.dump(output_data, f, indent=4)\n",
        "      print(\"Test set perplexity : {:.3f}\".format(test_perp))\n",
        "      print(\"Training over\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pOiIpRozUIDg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running LSTM model without dropout\n",
        "\n",
        "train, validation, test, words_len = data_load()\n",
        "train = minibatch(train,20,20)\n",
        "validation = minibatch(validation, 20, 20)\n",
        "test = minibatch(test,20,20)\n",
        "\n",
        "model_lstm = Model(words_len,200,2,0,0.1,'lstm') #LSTM model without dropout\n",
        "model_lstm.to(device)\n",
        "num_epochs = 16\n",
        "lr = 1\n",
        "epoch_threshold = 4\n",
        "main((train,validation,test), num_epochs, epoch_threshold, lr,model_lstm,\"LSTM\",'lstm')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6E9gN-3aepr",
        "outputId": "82b06042-9089-478d-dac6-639130e368d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "batch no = 0 / 2323, train loss = 9.215, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 6.274, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 5.971, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 5.770, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 5.291, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 5.438, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.501, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 5.193, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.921, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.458, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 5.368, lr = 1.000, \n",
            "Validation set perplexity : 181.405\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 181.195\n",
            "Training over\n",
            "Epoch 2/16\n",
            "batch no = 0 / 2323, train loss = 5.664, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 5.026, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 5.025, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.971, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.723, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.926, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.049, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.808, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.542, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.063, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.994, lr = 1.000, \n",
            "Validation set perplexity : 145.063\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 143.911\n",
            "Training over\n",
            "Epoch 3/16\n",
            "batch no = 0 / 2323, train loss = 5.396, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.717, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.731, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.666, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.462, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.740, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.815, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.687, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.394, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 4.770, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.718, lr = 1.000, \n",
            "Validation set perplexity : 134.250\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 132.310\n",
            "Training over\n",
            "Epoch 4/16\n",
            "batch no = 0 / 2323, train loss = 5.239, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.478, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.624, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.506, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.280, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.529, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.622, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.562, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.202, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 4.626, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.574, lr = 1.000, \n",
            "Validation set perplexity : 128.890\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 125.945\n",
            "Training over\n",
            "Epoch 5/16\n",
            "batch no = 0 / 2323, train loss = 5.129, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.257, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 4.342, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 4.254, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 3.990, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 4.261, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 4.343, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.275, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 3.861, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 4.265, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 4.230, lr = 0.500, \n",
            "Validation set perplexity : 118.161\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 114.937\n",
            "Training over\n",
            "Epoch 6/16\n",
            "batch no = 0 / 2323, train loss = 4.960, lr = 0.250, \n",
            "batch no = 232 / 2323, train loss = 4.045, lr = 0.250, \n",
            "batch no = 464 / 2323, train loss = 4.170, lr = 0.250, \n",
            "batch no = 696 / 2323, train loss = 4.116, lr = 0.250, \n",
            "batch no = 928 / 2323, train loss = 3.855, lr = 0.250, \n",
            "batch no = 1160 / 2323, train loss = 4.046, lr = 0.250, \n",
            "batch no = 1392 / 2323, train loss = 4.161, lr = 0.250, \n",
            "batch no = 1624 / 2323, train loss = 4.069, lr = 0.250, \n",
            "batch no = 1856 / 2323, train loss = 3.658, lr = 0.250, \n",
            "batch no = 2088 / 2323, train loss = 4.061, lr = 0.250, \n",
            "batch no = 2320 / 2323, train loss = 4.018, lr = 0.250, \n",
            "Validation set perplexity : 116.826\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 113.580\n",
            "Training over\n",
            "Epoch 7/16\n",
            "batch no = 0 / 2323, train loss = 4.850, lr = 0.125, \n",
            "batch no = 232 / 2323, train loss = 3.909, lr = 0.125, \n",
            "batch no = 464 / 2323, train loss = 4.040, lr = 0.125, \n",
            "batch no = 696 / 2323, train loss = 3.995, lr = 0.125, \n",
            "batch no = 928 / 2323, train loss = 3.752, lr = 0.125, \n",
            "batch no = 1160 / 2323, train loss = 3.951, lr = 0.125, \n",
            "batch no = 1392 / 2323, train loss = 4.042, lr = 0.125, \n",
            "batch no = 1624 / 2323, train loss = 3.952, lr = 0.125, \n",
            "batch no = 1856 / 2323, train loss = 3.525, lr = 0.125, \n",
            "batch no = 2088 / 2323, train loss = 3.934, lr = 0.125, \n",
            "batch no = 2320 / 2323, train loss = 3.904, lr = 0.125, \n",
            "Validation set perplexity : 117.578\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 114.058\n",
            "Training over\n",
            "Epoch 8/16\n",
            "batch no = 0 / 2323, train loss = 4.801, lr = 0.062, \n",
            "batch no = 232 / 2323, train loss = 3.842, lr = 0.062, \n",
            "batch no = 464 / 2323, train loss = 3.974, lr = 0.062, \n",
            "batch no = 696 / 2323, train loss = 3.932, lr = 0.062, \n",
            "batch no = 928 / 2323, train loss = 3.693, lr = 0.062, \n",
            "batch no = 1160 / 2323, train loss = 3.906, lr = 0.062, \n",
            "batch no = 1392 / 2323, train loss = 3.970, lr = 0.062, \n",
            "batch no = 1624 / 2323, train loss = 3.883, lr = 0.062, \n",
            "batch no = 1856 / 2323, train loss = 3.456, lr = 0.062, \n",
            "batch no = 2088 / 2323, train loss = 3.854, lr = 0.062, \n",
            "batch no = 2320 / 2323, train loss = 3.848, lr = 0.062, \n",
            "Validation set perplexity : 117.975\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 114.206\n",
            "Training over\n",
            "Epoch 9/16\n",
            "batch no = 0 / 2323, train loss = 4.769, lr = 0.031, \n",
            "batch no = 232 / 2323, train loss = 3.809, lr = 0.031, \n",
            "batch no = 464 / 2323, train loss = 3.946, lr = 0.031, \n",
            "batch no = 696 / 2323, train loss = 3.894, lr = 0.031, \n",
            "batch no = 928 / 2323, train loss = 3.652, lr = 0.031, \n",
            "batch no = 1160 / 2323, train loss = 3.892, lr = 0.031, \n",
            "batch no = 1392 / 2323, train loss = 3.930, lr = 0.031, \n",
            "batch no = 1624 / 2323, train loss = 3.842, lr = 0.031, \n",
            "batch no = 1856 / 2323, train loss = 3.426, lr = 0.031, \n",
            "batch no = 2088 / 2323, train loss = 3.810, lr = 0.031, \n",
            "batch no = 2320 / 2323, train loss = 3.816, lr = 0.031, \n",
            "Validation set perplexity : 117.989\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 114.113\n",
            "Training over\n",
            "Epoch 10/16\n",
            "batch no = 0 / 2323, train loss = 4.758, lr = 0.016, \n",
            "batch no = 232 / 2323, train loss = 3.792, lr = 0.016, \n",
            "batch no = 464 / 2323, train loss = 3.937, lr = 0.016, \n",
            "batch no = 696 / 2323, train loss = 3.866, lr = 0.016, \n",
            "batch no = 928 / 2323, train loss = 3.629, lr = 0.016, \n",
            "batch no = 1160 / 2323, train loss = 3.880, lr = 0.016, \n",
            "batch no = 1392 / 2323, train loss = 3.906, lr = 0.016, \n",
            "batch no = 1624 / 2323, train loss = 3.821, lr = 0.016, \n",
            "batch no = 1856 / 2323, train loss = 3.412, lr = 0.016, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running LSTM model with 30% dropout\n",
        "train, validation, test, words_len = data_load()\n",
        "model_lstm_dropout = Model(words_len,200,2,0.3,0.1,'lstm') #LSTM model with 30% dropout\n",
        "model_lstm_dropout.to(device)\n",
        "num_epochs = 16\n",
        "lr = 1\n",
        "epoch_threshold = 8\n",
        "main((train,validation,test), num_epochs, epoch_threshold, lr, model_lstm_dropout,\"LSTM_dropout\",'lstm')"
      ],
      "metadata": {
        "id": "eGAinqsfGp_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running GRU model without dropout\n",
        "\n",
        "model_gru = Model(words_len,200,2,0,0.1,'gru') #GRU model no dropout\n",
        "model_gru.to(device)\n",
        "num_epochs = 16\n",
        "lr = 0.5\n",
        "epoch_threshold = 6\n",
        "main((train,validation,test), num_epochs, epoch_threshold, lr, model_gru,\"GRU\",'gru')"
      ],
      "metadata": {
        "id": "6UDLA5psowJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru_dropout = Model(words_len,200,2,0.4,0.1,'gru') #GRU model 40% dropout\n",
        "model_gru_dropout.to(device)\n",
        "num_epochs = 20\n",
        "lr = 0.8\n",
        "epoch_threshold = 8\n",
        "main((train,validation,test), num_epochs, epoch_threshold, lr, model_gru_dropout,\"GRU_dropout\",'gru')"
      ],
      "metadata": {
        "id": "SVoYB8jVKtkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot perplexity vs epoch of test and train data from json files\n",
        "summary = {'LSTM Model' : [],\n",
        "           'LSTM Dropout Model': [],\n",
        "           'GRU Model': [],\n",
        "           'GRU Dropout Model':[]}\n",
        "with open('LSTM.json', mode='r', encoding='utf-8') as json_f:\n",
        "    results_dict = json.load(json_f)\n",
        "accuracies_regular_model = plt.figure()\n",
        "plt.plot(range(1, len(results_dict['train_perp']) + 1),\n",
        "             results_dict['train_perp'])\n",
        "plt.plot(range(1, len(results_dict['test_perp']) + 1),\n",
        "             results_dict['test_perp'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('Perplexity vs epoch for LSTM model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('perplexity')\n",
        "accuracies_regular_model.savefig('LSTM_perplexity_plot.png')\n",
        "plt.show()\n",
        "summary['LSTM Model'].append(min(results_dict['train_perp']))\n",
        "summary['LSTM Model'].append(min(results_dict['test_perp']))\n",
        "summary['LSTM Model'].append(min(results_dict['val_perp']))\n",
        "\n",
        "with open('LSTM_dropout.json', mode='r', encoding='utf-8') as json_f:\n",
        "    results_dict = json.load(json_f)\n",
        "accuracies_dropout_model = plt.figure()\n",
        "plt.plot(range(1, len(results_dict['train_perp']) + 1),\n",
        "             results_dict['train_perp'])\n",
        "plt.plot(range(1, len(results_dict['test_perp']) + 1),\n",
        "             results_dict['test_perp'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('Perplexity vs epoch for LSTM dropout model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('perplexity')\n",
        "accuracies_dropout_model.savefig('LSTM_dropout_perplexity_plot.png')\n",
        "plt.show()\n",
        "summary['LSTM Dropout Model'].append(min(results_dict['train_perp']))\n",
        "summary['LSTM Dropout Model'].append(min(results_dict['test_perp']))\n",
        "summary['LSTM Dropout Model'].append(min(results_dict['val_perp']))\n",
        "\n",
        "with open('GRU.json', mode='r', encoding='utf-8') as json_f:\n",
        "    results_dict = json.load(json_f)\n",
        "accuracies_wd_model = plt.figure()\n",
        "plt.plot(range(1, len(results_dict['train_perp']) + 1),\n",
        "             results_dict['train_perp'])\n",
        "plt.plot(range(1, len(results_dict['test_perp']) + 1),\n",
        "             results_dict['test_perp'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('Perplexity vs epoch for GRU model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('perplexity')\n",
        "accuracies_wd_model.savefig('GRU_perplexity_plot.png')\n",
        "plt.show()\n",
        "summary['GRU Model'].append(min(results_dict['train_perp']))\n",
        "summary['GRU Model'].append(min(results_dict['test_perp']))\n",
        "summary['GRU Model'].append(min(results_dict['val_perp']))\n",
        "\n",
        "with open('GRU_dropout.json', mode='r', encoding='utf-8') as json_f:\n",
        "    results_dict = json.load(json_f)\n",
        "accuracies_bn_model = plt.figure()\n",
        "plt.plot(range(1, len(results_dict['train_perp']) + 1),\n",
        "             results_dict['train_perp'])\n",
        "plt.plot(range(1, len(results_dict['test_perp']) + 1),\n",
        "             results_dict['test_perp'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('Perplexity vs epoch for GRU dropout model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('perplexity')\n",
        "accuracies_bn_model.savefig('GRU_dropout_perplexity_plot.png')\n",
        "plt.show()\n",
        "summary['GRU Dropout Model'].append(min(results_dict['train_perp']))\n",
        "summary['GRU Dropout Model'].append(min(results_dict['test_perp']))\n",
        "summary['GRU Dropout Model'].append(min(results_dict['val_perp']))"
      ],
      "metadata": {
        "id": "WDASKjPlp-4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"{:<20} {:<20} {:<20} {:<20}\".format('MODEL', 'VALIDATION', 'TRAIN', 'TEST'))\n",
        " \n",
        "#print max train and test accuracy for each model, summarize in a table\n",
        "for key, value in summary.items():\n",
        "    train,test,val = value\n",
        "    print(\"{:<20} {:<20} {:<20} {:<20}\".format(key, val, train, test))"
      ],
      "metadata": {
        "id": "yz9P4BOkr_Qj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}