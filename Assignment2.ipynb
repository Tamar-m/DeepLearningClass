{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tamar-m/DeepLearningClass/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code adapted from: https://github.com/ahmetumutdurmus/zaremba/blob/master/main.py\n",
        "\n",
        "See also relevant paper: https://arxiv.org/pdf/1409.2329.pdf)"
      ],
      "metadata": {
        "id": "OpRQ96iMLemu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y6jzjADXuExQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2ce899-5cdf-4e62-f725-21799f9005a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at Google_drive; to attempt to forcibly remount, call drive.mount(\"Google_drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('Google_drive')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data must be google drive in following path: /Deep Learning Class/Ex2Data\n",
        "def data_load():\n",
        "    with open(\"/content/Google_drive/MyDrive/Deep Learning Class/Ex2Data/ptb.train.txt\") as f:\n",
        "        file = f.read()\n",
        "        trn = file[1:].split(' ')\n",
        "    with open(\"/content/Google_drive/MyDrive/Deep Learning Class/Ex2Data/ptb.valid.txt\") as f:\n",
        "        file = f.read()\n",
        "        vld = file[1:].split(' ')\n",
        "    with open(\"/content/Google_drive/MyDrive/Deep Learning Class/Ex2Data/ptb.test.txt\") as f:\n",
        "        file = f.read()\n",
        "        tst = file[1:].split(' ')\n",
        "    words = sorted(set(trn))\n",
        "    char2ind = {c: i for i, c in enumerate(words)}\n",
        "    trn = [char2ind[c] for c in trn]\n",
        "    vld = [char2ind[c] for c in vld]\n",
        "    tst = [char2ind[c] for c in tst]\n",
        "    return np.array(trn).reshape(-1, 1), np.array(vld).reshape(-1, 1), np.array(tst).reshape(-1, 1), len(words)\n",
        "\n",
        "#create minibatches of the data\n",
        "def minibatch(data, batch_size, seq_length):\n",
        "    data = torch.tensor(data, dtype = torch.int64)\n",
        "    num_batches = data.size(0)//batch_size\n",
        "    data = data[:num_batches*batch_size]\n",
        "    data=data.view(batch_size,-1)\n",
        "    dataset = []\n",
        "    for i in range(0,data.size(1)-1,seq_length):\n",
        "        seqlen=int(np.min([seq_length,data.size(1)-1-i]))\n",
        "        if seqlen<data.size(1)-1-i:\n",
        "            x=data[:,i:i+seqlen].transpose(1, 0)\n",
        "            y=data[:,i+1:i+seqlen+1].transpose(1, 0)\n",
        "            dataset.append((x, y))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "ezJmXB7IuHcD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding module\n",
        "class Embed(nn.Module): #embedding is efficient when we have a large number of input features\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.W = nn.Parameter(torch.Tensor(vocab_size, embed_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.W[x]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Embedding(vocab: {}, embedding: {})\".format(self.vocab_size, self.embed_size)\n",
        "\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "        self.b = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = torch.addmm(self.b, x.view(-1, x.size(2)), self.W.t())\n",
        "        return z\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"FC(input: {}, output: {})\".format(self.input_size, self.hidden_size)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, layer_num, dropout, winit, base):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layer_num = layer_num\n",
        "        self.winit = winit\n",
        "        self.embed = Embed(vocab_size, hidden_size)\n",
        "        if base == 'gru':\n",
        "          self.rnns = [nn.GRU(hidden_size, hidden_size) for i in range(layer_num)] #two GRU layers with 200 units per layer\n",
        "        else:\n",
        "          self.rnns = [nn.LSTM(hidden_size, hidden_size) for i in range(layer_num)] #two LSTM layers with 200 units per layer\n",
        "        self.rnns = nn.ModuleList(self.rnns)\n",
        "        self.fc = Linear(hidden_size, vocab_size) \n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.reset_parameters() #weights are reset uniformly when initializing the model\n",
        "        \n",
        "    def reset_parameters(self): #LSTM hidden layers weights initialized uniformly between -0.1 to 0.1\n",
        "        for param in self.parameters():\n",
        "            nn.init.uniform_(param, -self.winit, self.winit) \n",
        "            \n",
        "    def state_init(self, batch_size,base): #hidden states are initialized to zero, input of gru is h_0 and input of lstm is (h_0, c_0) \n",
        "        dev = next(self.parameters()).device\n",
        "        if base == 'gru':\n",
        "          states = [(torch.zeros(1, batch_size, layer.hidden_size, device = dev)) for layer in self.rnns]\n",
        "        else:\n",
        "          states = [(torch.zeros(1, batch_size, layer.hidden_size, device = dev), torch.zeros(1, batch_size, layer.hidden_size, device = dev)) for layer in self.rnns]\n",
        "        return states \n",
        "\n",
        "    # detach to complete truncated backpropagation through time (BPTT)\n",
        "    def detach(self, states, base):\n",
        "      if base == 'lstm':\n",
        "        return [(h.detach(), c.detach()) for (h,c) in states] # returns tensors that don't require a gradient\n",
        "      else:\n",
        "        return [h.detach() for h in states] #gru layers input of shape h_0\n",
        "\n",
        "    def forward(self, x, states):\n",
        "        x = self.embed(x)\n",
        "        x = self.dropout(x)\n",
        "        for i, rnn in enumerate(self.rnns):\n",
        "            x, states[i] = rnn(x, states[i])\n",
        "            x = self.dropout(x)\n",
        "        scores = self.fc(x)\n",
        "        return scores, states"
      ],
      "metadata": {
        "id": "w6j8oL5x5mbg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we define the loss (negative log likelihood) and perplexity"
      ],
      "metadata": {
        "id": "eLcQ38-oPJyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nll_loss(scores, y):\n",
        "    batch_size = y.size(1)\n",
        "    expscores = scores.exp()\n",
        "    probabilities = expscores / expscores.sum(1, keepdim = True)\n",
        "    answerprobs = probabilities[range(len(y.reshape(-1))), y.reshape(-1)]\n",
        "    return torch.mean(-torch.log(answerprobs) * batch_size)\n",
        "\n",
        "\n",
        "def perplexity(data,model,base):\n",
        "  with torch.no_grad():\n",
        "    losses = []\n",
        "    states = model.state_init(20,base)\n",
        "    for x,y in data:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      scores, states = model(x,states)\n",
        "      loss = nll_loss(scores, y)\n",
        "      losses.append(loss.data.item()/20)\n",
        "    return np.exp(np.mean(losses))"
      ],
      "metadata": {
        "id": "wxbdG19pYu8u"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(data, model, lr, max_norm,base):\n",
        "    trn, vld, tst = data\n",
        "    total_words = 0\n",
        "    states = model.state_init(20,base) # batch size of 20 according to article\n",
        "    model.train()\n",
        "    for i, (x, y) in enumerate(trn):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        total_words += x.numel()\n",
        "        model.zero_grad()\n",
        "        states = model.detach(states, base)\n",
        "        scores, states = model(x, states)\n",
        "        loss = nll_loss(scores, y)\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "            for param in model.parameters():\n",
        "                param -= lr * param.grad\n",
        "        if i % (len(trn)//10) == 0:\n",
        "            print(\"batch no = {:d} / {:d}, \".format(i, len(trn)) +\n",
        "                  \"train loss = {:.3f}, \".format(loss.item()/20) + #calculate loss\n",
        "                  \"lr = {:.3f}, \".format(lr))\n",
        "    model.eval()\n",
        "    val_perp = perplexity(vld, model,base)\n",
        "    print(\"Validation set perplexity : {:.3f}\".format(val_perp))\n",
        "    print(\"*************************************************\\n\")\n",
        "    return val_perp"
      ],
      "metadata": {
        "id": "i5Mze4n--UyX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(data,epochs,epoch_threshold,lr,model,model_name,base,lr_factor):\n",
        "  train, validation, test, word_len = data_load()\n",
        "  train = minibatch(train,20,20)\n",
        "  validation = minibatch(validation, 20, 20)\n",
        "  test = minibatch(test,20,20)\n",
        "  output_data = {\"model\": model_name,\n",
        "                 \"train_perp\": [],\n",
        "                 \"test_perp\":[],\n",
        "                 \"val_perp\":[]\n",
        "                 }\n",
        "  best_perp = np.Inf\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      if epoch > epoch_threshold: \n",
        "        lr = lr / lr_factor # after epoch threshold, learning rate decreased by a factor for each epoch\n",
        "      print(f'Epoch {epoch}/{epochs}')\n",
        "      val_perp = train_one_epoch((train,validation,test),model,lr,5,base)   \n",
        "      model.eval()   \n",
        "      test_perp = perplexity(test, model,base)\n",
        "      train_perp = perplexity(train, model,base)\n",
        "      output_data[\"train_perp\"].append(train_perp)\n",
        "      output_data[\"test_perp\"].append(test_perp)\n",
        "      output_data[\"val_perp\"].append(val_perp)\n",
        "      filename = model_name + '.json'\n",
        "      if test_perp < best_perp:\n",
        "        state = {\n",
        "                'model': model.state_dict(),\n",
        "                'train_perp': train_perp,\n",
        "                'test_perp': test_perp,\n",
        "                'epoch': epoch,\n",
        "                }\n",
        "        best_perp = test_perp\n",
        "        torch.save(state, model_name + '.pt')\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "          json.dump(output_data, f, indent=4)\n",
        "      print(\"Test set perplexity : {:.3f}\".format(test_perp))\n",
        "      print(\"Training over\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pOiIpRozUIDg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running LSTM model without dropout\n",
        "\n",
        "train, validation, test, words_len = data_load()\n",
        "train = minibatch(train,20,20)\n",
        "validation = minibatch(validation, 20, 20)\n",
        "test = minibatch(test,20,20)\n",
        "\n",
        "model_lstm = Model(words_len,200,2,0,0.1,'lstm') #LSTM model without dropout\n",
        "model_lstm.to(device)\n",
        "num_epochs = 16\n",
        "lr = 1\n",
        "epoch_threshold = 4\n",
        "main((train,validation,test), num_epochs, epoch_threshold, lr,model_lstm,\"LSTM\",'lstm',2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6E9gN-3aepr",
        "outputId": "d4241663-0771-4889-e3df-cf27aec10165"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "batch no = 0 / 2323, train loss = 9.208, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 6.330, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 5.951, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 5.708, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 5.313, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 5.399, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.509, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 5.219, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.925, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.486, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 5.360, lr = 1.000, \n",
            "Validation set perplexity : 186.534\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 184.924\n",
            "Training over\n",
            "Epoch 2/16\n",
            "batch no = 0 / 2323, train loss = 5.702, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 5.022, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 5.013, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.972, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.711, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.940, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.108, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.834, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.542, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.046, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 5.012, lr = 1.000, \n",
            "Validation set perplexity : 146.313\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 144.436\n",
            "Training over\n",
            "Epoch 3/16\n",
            "batch no = 0 / 2323, train loss = 5.455, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.634, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.727, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.692, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.467, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.769, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.856, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.626, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.385, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 4.780, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.741, lr = 1.000, \n",
            "Validation set perplexity : 134.602\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 132.927\n",
            "Training over\n",
            "Epoch 4/16\n",
            "batch no = 0 / 2323, train loss = 5.302, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.407, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.580, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.487, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.222, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.632, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.667, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.508, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.251, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 4.569, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.571, lr = 1.000, \n",
            "Validation set perplexity : 129.950\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 127.822\n",
            "Training over\n",
            "Epoch 5/16\n",
            "batch no = 0 / 2323, train loss = 5.158, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.222, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 4.300, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 4.249, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.009, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 4.336, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 4.379, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.295, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 3.972, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 4.228, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 4.263, lr = 0.500, \n",
            "Validation set perplexity : 120.273\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 117.651\n",
            "Training over\n",
            "Epoch 6/16\n",
            "batch no = 0 / 2323, train loss = 4.961, lr = 0.250, \n",
            "batch no = 232 / 2323, train loss = 3.987, lr = 0.250, \n",
            "batch no = 464 / 2323, train loss = 4.121, lr = 0.250, \n",
            "batch no = 696 / 2323, train loss = 4.056, lr = 0.250, \n",
            "batch no = 928 / 2323, train loss = 3.789, lr = 0.250, \n",
            "batch no = 1160 / 2323, train loss = 4.156, lr = 0.250, \n",
            "batch no = 1392 / 2323, train loss = 4.161, lr = 0.250, \n",
            "batch no = 1624 / 2323, train loss = 4.095, lr = 0.250, \n",
            "batch no = 1856 / 2323, train loss = 3.789, lr = 0.250, \n",
            "batch no = 2088 / 2323, train loss = 4.016, lr = 0.250, \n",
            "batch no = 2320 / 2323, train loss = 4.051, lr = 0.250, \n",
            "Validation set perplexity : 118.479\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 115.309\n",
            "Training over\n",
            "Epoch 7/16\n",
            "batch no = 0 / 2323, train loss = 4.882, lr = 0.125, \n",
            "batch no = 232 / 2323, train loss = 3.884, lr = 0.125, \n",
            "batch no = 464 / 2323, train loss = 4.002, lr = 0.125, \n",
            "batch no = 696 / 2323, train loss = 3.952, lr = 0.125, \n",
            "batch no = 928 / 2323, train loss = 3.677, lr = 0.125, \n",
            "batch no = 1160 / 2323, train loss = 4.074, lr = 0.125, \n",
            "batch no = 1392 / 2323, train loss = 4.032, lr = 0.125, \n",
            "batch no = 1624 / 2323, train loss = 3.994, lr = 0.125, \n",
            "batch no = 1856 / 2323, train loss = 3.665, lr = 0.125, \n",
            "batch no = 2088 / 2323, train loss = 3.880, lr = 0.125, \n",
            "batch no = 2320 / 2323, train loss = 3.912, lr = 0.125, \n",
            "Validation set perplexity : 118.121\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 114.876\n",
            "Training over\n",
            "Epoch 8/16\n",
            "batch no = 0 / 2323, train loss = 4.834, lr = 0.062, \n",
            "batch no = 232 / 2323, train loss = 3.822, lr = 0.062, \n",
            "batch no = 464 / 2323, train loss = 3.954, lr = 0.062, \n",
            "batch no = 696 / 2323, train loss = 3.901, lr = 0.062, \n",
            "batch no = 928 / 2323, train loss = 3.609, lr = 0.062, \n",
            "batch no = 1160 / 2323, train loss = 4.021, lr = 0.062, \n",
            "batch no = 1392 / 2323, train loss = 3.968, lr = 0.062, \n",
            "batch no = 1624 / 2323, train loss = 3.932, lr = 0.062, \n",
            "batch no = 1856 / 2323, train loss = 3.596, lr = 0.062, \n",
            "batch no = 2088 / 2323, train loss = 3.816, lr = 0.062, \n",
            "batch no = 2320 / 2323, train loss = 3.844, lr = 0.062, \n",
            "Validation set perplexity : 118.133\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 114.629\n",
            "Training over\n",
            "Epoch 9/16\n",
            "batch no = 0 / 2323, train loss = 4.806, lr = 0.031, \n",
            "batch no = 232 / 2323, train loss = 3.778, lr = 0.031, \n",
            "batch no = 464 / 2323, train loss = 3.929, lr = 0.031, \n",
            "batch no = 696 / 2323, train loss = 3.869, lr = 0.031, \n",
            "batch no = 928 / 2323, train loss = 3.576, lr = 0.031, \n",
            "batch no = 1160 / 2323, train loss = 3.990, lr = 0.031, \n",
            "batch no = 1392 / 2323, train loss = 3.932, lr = 0.031, \n",
            "batch no = 1624 / 2323, train loss = 3.895, lr = 0.031, \n",
            "batch no = 1856 / 2323, train loss = 3.554, lr = 0.031, \n",
            "batch no = 2088 / 2323, train loss = 3.773, lr = 0.031, \n",
            "batch no = 2320 / 2323, train loss = 3.809, lr = 0.031, \n",
            "Validation set perplexity : 118.149\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 114.399\n",
            "Training over\n",
            "Epoch 10/16\n",
            "batch no = 0 / 2323, train loss = 4.799, lr = 0.016, \n",
            "batch no = 232 / 2323, train loss = 3.750, lr = 0.016, \n",
            "batch no = 464 / 2323, train loss = 3.913, lr = 0.016, \n",
            "batch no = 696 / 2323, train loss = 3.852, lr = 0.016, \n",
            "batch no = 928 / 2323, train loss = 3.560, lr = 0.016, \n",
            "batch no = 1160 / 2323, train loss = 3.972, lr = 0.016, \n",
            "batch no = 1392 / 2323, train loss = 3.914, lr = 0.016, \n",
            "batch no = 1624 / 2323, train loss = 3.875, lr = 0.016, \n",
            "batch no = 1856 / 2323, train loss = 3.534, lr = 0.016, \n",
            "batch no = 2088 / 2323, train loss = 3.748, lr = 0.016, \n",
            "batch no = 2320 / 2323, train loss = 3.789, lr = 0.016, \n",
            "Validation set perplexity : 117.968\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 114.089\n",
            "Training over\n",
            "Epoch 11/16\n",
            "batch no = 0 / 2323, train loss = 4.794, lr = 0.008, \n",
            "batch no = 232 / 2323, train loss = 3.734, lr = 0.008, \n",
            "batch no = 464 / 2323, train loss = 3.902, lr = 0.008, \n",
            "batch no = 696 / 2323, train loss = 3.842, lr = 0.008, \n",
            "batch no = 928 / 2323, train loss = 3.550, lr = 0.008, \n",
            "batch no = 1160 / 2323, train loss = 3.961, lr = 0.008, \n",
            "batch no = 1392 / 2323, train loss = 3.902, lr = 0.008, \n",
            "batch no = 1624 / 2323, train loss = 3.864, lr = 0.008, \n",
            "batch no = 1856 / 2323, train loss = 3.526, lr = 0.008, \n",
            "batch no = 2088 / 2323, train loss = 3.733, lr = 0.008, \n",
            "batch no = 2320 / 2323, train loss = 3.779, lr = 0.008, \n",
            "Validation set perplexity : 117.665\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 113.758\n",
            "Training over\n",
            "Epoch 12/16\n",
            "batch no = 0 / 2323, train loss = 4.791, lr = 0.004, \n",
            "batch no = 232 / 2323, train loss = 3.725, lr = 0.004, \n",
            "batch no = 464 / 2323, train loss = 3.896, lr = 0.004, \n",
            "batch no = 696 / 2323, train loss = 3.836, lr = 0.004, \n",
            "batch no = 928 / 2323, train loss = 3.545, lr = 0.004, \n",
            "batch no = 1160 / 2323, train loss = 3.955, lr = 0.004, \n",
            "batch no = 1392 / 2323, train loss = 3.894, lr = 0.004, \n",
            "batch no = 1624 / 2323, train loss = 3.858, lr = 0.004, \n",
            "batch no = 1856 / 2323, train loss = 3.521, lr = 0.004, \n",
            "batch no = 2088 / 2323, train loss = 3.725, lr = 0.004, \n",
            "batch no = 2320 / 2323, train loss = 3.773, lr = 0.004, \n",
            "Validation set perplexity : 117.385\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 113.476\n",
            "Training over\n",
            "Epoch 13/16\n",
            "batch no = 0 / 2323, train loss = 4.789, lr = 0.002, \n",
            "batch no = 232 / 2323, train loss = 3.721, lr = 0.002, \n",
            "batch no = 464 / 2323, train loss = 3.892, lr = 0.002, \n",
            "batch no = 696 / 2323, train loss = 3.833, lr = 0.002, \n",
            "batch no = 928 / 2323, train loss = 3.543, lr = 0.002, \n",
            "batch no = 1160 / 2323, train loss = 3.953, lr = 0.002, \n",
            "batch no = 1392 / 2323, train loss = 3.889, lr = 0.002, \n",
            "batch no = 1624 / 2323, train loss = 3.854, lr = 0.002, \n",
            "batch no = 1856 / 2323, train loss = 3.519, lr = 0.002, \n",
            "batch no = 2088 / 2323, train loss = 3.720, lr = 0.002, \n",
            "batch no = 2320 / 2323, train loss = 3.769, lr = 0.002, \n",
            "Validation set perplexity : 117.192\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 113.281\n",
            "Training over\n",
            "Epoch 14/16\n",
            "batch no = 0 / 2323, train loss = 4.787, lr = 0.001, \n",
            "batch no = 232 / 2323, train loss = 3.719, lr = 0.001, \n",
            "batch no = 464 / 2323, train loss = 3.890, lr = 0.001, \n",
            "batch no = 696 / 2323, train loss = 3.831, lr = 0.001, \n",
            "batch no = 928 / 2323, train loss = 3.541, lr = 0.001, \n",
            "batch no = 1160 / 2323, train loss = 3.952, lr = 0.001, \n",
            "batch no = 1392 / 2323, train loss = 3.887, lr = 0.001, \n",
            "batch no = 1624 / 2323, train loss = 3.852, lr = 0.001, \n",
            "batch no = 1856 / 2323, train loss = 3.518, lr = 0.001, \n",
            "batch no = 2088 / 2323, train loss = 3.718, lr = 0.001, \n",
            "batch no = 2320 / 2323, train loss = 3.767, lr = 0.001, \n",
            "Validation set perplexity : 117.090\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 113.177\n",
            "Training over\n",
            "Epoch 15/16\n",
            "batch no = 0 / 2323, train loss = 4.787, lr = 0.000, \n",
            "batch no = 232 / 2323, train loss = 3.719, lr = 0.000, \n",
            "batch no = 464 / 2323, train loss = 3.889, lr = 0.000, \n",
            "batch no = 696 / 2323, train loss = 3.830, lr = 0.000, \n",
            "batch no = 928 / 2323, train loss = 3.540, lr = 0.000, \n",
            "batch no = 1160 / 2323, train loss = 3.951, lr = 0.000, \n",
            "batch no = 1392 / 2323, train loss = 3.886, lr = 0.000, \n",
            "batch no = 1624 / 2323, train loss = 3.851, lr = 0.000, \n",
            "batch no = 1856 / 2323, train loss = 3.517, lr = 0.000, \n",
            "batch no = 2088 / 2323, train loss = 3.717, lr = 0.000, \n",
            "batch no = 2320 / 2323, train loss = 3.766, lr = 0.000, \n",
            "Validation set perplexity : 117.039\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 113.127\n",
            "Training over\n",
            "Epoch 16/16\n",
            "batch no = 0 / 2323, train loss = 4.786, lr = 0.000, \n",
            "batch no = 232 / 2323, train loss = 3.718, lr = 0.000, \n",
            "batch no = 464 / 2323, train loss = 3.889, lr = 0.000, \n",
            "batch no = 696 / 2323, train loss = 3.829, lr = 0.000, \n",
            "batch no = 928 / 2323, train loss = 3.540, lr = 0.000, \n",
            "batch no = 1160 / 2323, train loss = 3.951, lr = 0.000, \n",
            "batch no = 1392 / 2323, train loss = 3.885, lr = 0.000, \n",
            "batch no = 1624 / 2323, train loss = 3.850, lr = 0.000, \n",
            "batch no = 1856 / 2323, train loss = 3.517, lr = 0.000, \n",
            "batch no = 2088 / 2323, train loss = 3.717, lr = 0.000, \n",
            "batch no = 2320 / 2323, train loss = 3.765, lr = 0.000, \n",
            "Validation set perplexity : 117.014\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 113.103\n",
            "Training over\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running LSTM model with 30% dropout\n",
        "train, validation, test, words_len = data_load()\n",
        "model_lstm_dropout = Model(words_len,200,2,0.3,0.1,'lstm') #LSTM model with 30% dropout\n",
        "model_lstm_dropout.to(device)\n",
        "num_epochs = 30\n",
        "lr = 1\n",
        "lr_factor = 1.5\n",
        "epoch_threshold = 8\n",
        "main((train,validation,test), num_epochs, epoch_threshold, lr, model_lstm_dropout,\"LSTM_dropout\",'lstm',lr_factor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGAinqsfGp_T",
        "outputId": "bb9c39fd-23f6-4414-a719-d6afd36a9bd1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "batch no = 0 / 2323, train loss = 9.217, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 6.367, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 6.074, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 5.824, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 5.483, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 5.577, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.669, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 5.392, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 5.104, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.639, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 5.548, lr = 1.000, \n",
            "Validation set perplexity : 197.622\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 194.153\n",
            "Training over\n",
            "Epoch 2/30\n",
            "batch no = 0 / 2323, train loss = 5.872, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 5.197, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 5.193, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 5.238, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.952, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 5.149, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.301, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 5.090, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.793, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.354, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 5.277, lr = 1.000, \n",
            "Validation set perplexity : 156.828\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 154.581\n",
            "Training over\n",
            "Epoch 3/30\n",
            "batch no = 0 / 2323, train loss = 5.682, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.988, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 5.088, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 5.025, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.741, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 5.033, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.179, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.877, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.659, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.175, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 5.117, lr = 1.000, \n",
            "Validation set perplexity : 140.740\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 138.436\n",
            "Training over\n",
            "Epoch 4/30\n",
            "batch no = 0 / 2323, train loss = 5.563, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.838, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.910, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.838, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.676, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.839, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.075, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.858, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.589, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.182, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.992, lr = 1.000, \n",
            "Validation set perplexity : 131.538\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 128.697\n",
            "Training over\n",
            "Epoch 5/30\n",
            "batch no = 0 / 2323, train loss = 5.491, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.826, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.857, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.752, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.489, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.853, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.997, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.763, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.520, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.004, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.928, lr = 1.000, \n",
            "Validation set perplexity : 126.495\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 124.551\n",
            "Training over\n",
            "Epoch 6/30\n",
            "batch no = 0 / 2323, train loss = 5.450, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.770, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.735, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.725, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.609, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.792, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.904, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.771, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.479, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 4.940, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.896, lr = 1.000, \n",
            "Validation set perplexity : 122.289\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 120.216\n",
            "Training over\n",
            "Epoch 7/30\n",
            "batch no = 0 / 2323, train loss = 5.382, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.617, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.730, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.692, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.427, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.750, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.967, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.671, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.513, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 4.956, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.788, lr = 1.000, \n",
            "Validation set perplexity : 119.755\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 117.031\n",
            "Training over\n",
            "Epoch 8/30\n",
            "batch no = 0 / 2323, train loss = 5.409, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.619, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.723, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.647, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.493, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.690, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.908, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.688, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.466, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 4.913, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.771, lr = 1.000, \n",
            "Validation set perplexity : 118.570\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 115.623\n",
            "Training over\n",
            "Epoch 9/30\n",
            "batch no = 0 / 2323, train loss = 5.303, lr = 0.667, \n",
            "batch no = 232 / 2323, train loss = 4.635, lr = 0.667, \n",
            "batch no = 464 / 2323, train loss = 4.682, lr = 0.667, \n",
            "batch no = 696 / 2323, train loss = 4.562, lr = 0.667, \n",
            "batch no = 928 / 2323, train loss = 4.478, lr = 0.667, \n",
            "batch no = 1160 / 2323, train loss = 4.599, lr = 0.667, \n",
            "batch no = 1392 / 2323, train loss = 4.694, lr = 0.667, \n",
            "batch no = 1624 / 2323, train loss = 4.593, lr = 0.667, \n",
            "batch no = 1856 / 2323, train loss = 4.371, lr = 0.667, \n",
            "batch no = 2088 / 2323, train loss = 4.702, lr = 0.667, \n",
            "batch no = 2320 / 2323, train loss = 4.645, lr = 0.667, \n",
            "Validation set perplexity : 110.952\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 108.557\n",
            "Training over\n",
            "Epoch 10/30\n",
            "batch no = 0 / 2323, train loss = 5.334, lr = 0.444, \n",
            "batch no = 232 / 2323, train loss = 4.541, lr = 0.444, \n",
            "batch no = 464 / 2323, train loss = 4.563, lr = 0.444, \n",
            "batch no = 696 / 2323, train loss = 4.536, lr = 0.444, \n",
            "batch no = 928 / 2323, train loss = 4.322, lr = 0.444, \n",
            "batch no = 1160 / 2323, train loss = 4.592, lr = 0.444, \n",
            "batch no = 1392 / 2323, train loss = 4.605, lr = 0.444, \n",
            "batch no = 1624 / 2323, train loss = 4.481, lr = 0.444, \n",
            "batch no = 1856 / 2323, train loss = 4.168, lr = 0.444, \n",
            "batch no = 2088 / 2323, train loss = 4.602, lr = 0.444, \n",
            "batch no = 2320 / 2323, train loss = 4.657, lr = 0.444, \n",
            "Validation set perplexity : 106.299\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 103.580\n",
            "Training over\n",
            "Epoch 11/30\n",
            "batch no = 0 / 2323, train loss = 5.258, lr = 0.296, \n",
            "batch no = 232 / 2323, train loss = 4.488, lr = 0.296, \n",
            "batch no = 464 / 2323, train loss = 4.496, lr = 0.296, \n",
            "batch no = 696 / 2323, train loss = 4.390, lr = 0.296, \n",
            "batch no = 928 / 2323, train loss = 4.306, lr = 0.296, \n",
            "batch no = 1160 / 2323, train loss = 4.494, lr = 0.296, \n",
            "batch no = 1392 / 2323, train loss = 4.571, lr = 0.296, \n",
            "batch no = 1624 / 2323, train loss = 4.410, lr = 0.296, \n",
            "batch no = 1856 / 2323, train loss = 4.154, lr = 0.296, \n",
            "batch no = 2088 / 2323, train loss = 4.491, lr = 0.296, \n",
            "batch no = 2320 / 2323, train loss = 4.524, lr = 0.296, \n",
            "Validation set perplexity : 103.937\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 100.804\n",
            "Training over\n",
            "Epoch 12/30\n",
            "batch no = 0 / 2323, train loss = 5.192, lr = 0.198, \n",
            "batch no = 232 / 2323, train loss = 4.369, lr = 0.198, \n",
            "batch no = 464 / 2323, train loss = 4.450, lr = 0.198, \n",
            "batch no = 696 / 2323, train loss = 4.388, lr = 0.198, \n",
            "batch no = 928 / 2323, train loss = 4.244, lr = 0.198, \n",
            "batch no = 1160 / 2323, train loss = 4.452, lr = 0.198, \n",
            "batch no = 1392 / 2323, train loss = 4.540, lr = 0.198, \n",
            "batch no = 1624 / 2323, train loss = 4.411, lr = 0.198, \n",
            "batch no = 1856 / 2323, train loss = 4.121, lr = 0.198, \n",
            "batch no = 2088 / 2323, train loss = 4.497, lr = 0.198, \n",
            "batch no = 2320 / 2323, train loss = 4.529, lr = 0.198, \n",
            "Validation set perplexity : 101.875\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 98.700\n",
            "Training over\n",
            "Epoch 13/30\n",
            "batch no = 0 / 2323, train loss = 5.147, lr = 0.132, \n",
            "batch no = 232 / 2323, train loss = 4.378, lr = 0.132, \n",
            "batch no = 464 / 2323, train loss = 4.318, lr = 0.132, \n",
            "batch no = 696 / 2323, train loss = 4.344, lr = 0.132, \n",
            "batch no = 928 / 2323, train loss = 4.254, lr = 0.132, \n",
            "batch no = 1160 / 2323, train loss = 4.448, lr = 0.132, \n",
            "batch no = 1392 / 2323, train loss = 4.530, lr = 0.132, \n",
            "batch no = 1624 / 2323, train loss = 4.315, lr = 0.132, \n",
            "batch no = 1856 / 2323, train loss = 4.067, lr = 0.132, \n",
            "batch no = 2088 / 2323, train loss = 4.472, lr = 0.132, \n",
            "batch no = 2320 / 2323, train loss = 4.443, lr = 0.132, \n",
            "Validation set perplexity : 100.646\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 97.398\n",
            "Training over\n",
            "Epoch 14/30\n",
            "batch no = 0 / 2323, train loss = 5.095, lr = 0.088, \n",
            "batch no = 232 / 2323, train loss = 4.295, lr = 0.088, \n",
            "batch no = 464 / 2323, train loss = 4.361, lr = 0.088, \n",
            "batch no = 696 / 2323, train loss = 4.373, lr = 0.088, \n",
            "batch no = 928 / 2323, train loss = 4.190, lr = 0.088, \n",
            "batch no = 1160 / 2323, train loss = 4.319, lr = 0.088, \n",
            "batch no = 1392 / 2323, train loss = 4.486, lr = 0.088, \n",
            "batch no = 1624 / 2323, train loss = 4.389, lr = 0.088, \n",
            "batch no = 1856 / 2323, train loss = 4.088, lr = 0.088, \n",
            "batch no = 2088 / 2323, train loss = 4.449, lr = 0.088, \n",
            "batch no = 2320 / 2323, train loss = 4.394, lr = 0.088, \n",
            "Validation set perplexity : 99.701\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.335\n",
            "Training over\n",
            "Epoch 15/30\n",
            "batch no = 0 / 2323, train loss = 5.145, lr = 0.059, \n",
            "batch no = 232 / 2323, train loss = 4.308, lr = 0.059, \n",
            "batch no = 464 / 2323, train loss = 4.359, lr = 0.059, \n",
            "batch no = 696 / 2323, train loss = 4.321, lr = 0.059, \n",
            "batch no = 928 / 2323, train loss = 4.131, lr = 0.059, \n",
            "batch no = 1160 / 2323, train loss = 4.413, lr = 0.059, \n",
            "batch no = 1392 / 2323, train loss = 4.431, lr = 0.059, \n",
            "batch no = 1624 / 2323, train loss = 4.341, lr = 0.059, \n",
            "batch no = 1856 / 2323, train loss = 3.974, lr = 0.059, \n",
            "batch no = 2088 / 2323, train loss = 4.412, lr = 0.059, \n",
            "batch no = 2320 / 2323, train loss = 4.439, lr = 0.059, \n",
            "Validation set perplexity : 99.125\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.692\n",
            "Training over\n",
            "Epoch 16/30\n",
            "batch no = 0 / 2323, train loss = 5.106, lr = 0.039, \n",
            "batch no = 232 / 2323, train loss = 4.397, lr = 0.039, \n",
            "batch no = 464 / 2323, train loss = 4.354, lr = 0.039, \n",
            "batch no = 696 / 2323, train loss = 4.273, lr = 0.039, \n",
            "batch no = 928 / 2323, train loss = 4.217, lr = 0.039, \n",
            "batch no = 1160 / 2323, train loss = 4.405, lr = 0.039, \n",
            "batch no = 1392 / 2323, train loss = 4.463, lr = 0.039, \n",
            "batch no = 1624 / 2323, train loss = 4.318, lr = 0.039, \n",
            "batch no = 1856 / 2323, train loss = 4.039, lr = 0.039, \n",
            "batch no = 2088 / 2323, train loss = 4.471, lr = 0.039, \n",
            "batch no = 2320 / 2323, train loss = 4.451, lr = 0.039, \n",
            "Validation set perplexity : 98.849\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.359\n",
            "Training over\n",
            "Epoch 17/30\n",
            "batch no = 0 / 2323, train loss = 5.150, lr = 0.026, \n",
            "batch no = 232 / 2323, train loss = 4.260, lr = 0.026, \n",
            "batch no = 464 / 2323, train loss = 4.364, lr = 0.026, \n",
            "batch no = 696 / 2323, train loss = 4.335, lr = 0.026, \n",
            "batch no = 928 / 2323, train loss = 4.131, lr = 0.026, \n",
            "batch no = 1160 / 2323, train loss = 4.421, lr = 0.026, \n",
            "batch no = 1392 / 2323, train loss = 4.500, lr = 0.026, \n",
            "batch no = 1624 / 2323, train loss = 4.312, lr = 0.026, \n",
            "batch no = 1856 / 2323, train loss = 3.987, lr = 0.026, \n",
            "batch no = 2088 / 2323, train loss = 4.423, lr = 0.026, \n",
            "batch no = 2320 / 2323, train loss = 4.401, lr = 0.026, \n",
            "Validation set perplexity : 98.628\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.082\n",
            "Training over\n",
            "Epoch 18/30\n",
            "batch no = 0 / 2323, train loss = 5.097, lr = 0.017, \n",
            "batch no = 232 / 2323, train loss = 4.274, lr = 0.017, \n",
            "batch no = 464 / 2323, train loss = 4.316, lr = 0.017, \n",
            "batch no = 696 / 2323, train loss = 4.301, lr = 0.017, \n",
            "batch no = 928 / 2323, train loss = 4.110, lr = 0.017, \n",
            "batch no = 1160 / 2323, train loss = 4.325, lr = 0.017, \n",
            "batch no = 1392 / 2323, train loss = 4.494, lr = 0.017, \n",
            "batch no = 1624 / 2323, train loss = 4.290, lr = 0.017, \n",
            "batch no = 1856 / 2323, train loss = 4.058, lr = 0.017, \n",
            "batch no = 2088 / 2323, train loss = 4.414, lr = 0.017, \n",
            "batch no = 2320 / 2323, train loss = 4.406, lr = 0.017, \n",
            "Validation set perplexity : 98.479\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.932\n",
            "Training over\n",
            "Epoch 19/30\n",
            "batch no = 0 / 2323, train loss = 5.111, lr = 0.012, \n",
            "batch no = 232 / 2323, train loss = 4.244, lr = 0.012, \n",
            "batch no = 464 / 2323, train loss = 4.319, lr = 0.012, \n",
            "batch no = 696 / 2323, train loss = 4.360, lr = 0.012, \n",
            "batch no = 928 / 2323, train loss = 4.194, lr = 0.012, \n",
            "batch no = 1160 / 2323, train loss = 4.486, lr = 0.012, \n",
            "batch no = 1392 / 2323, train loss = 4.460, lr = 0.012, \n",
            "batch no = 1624 / 2323, train loss = 4.284, lr = 0.012, \n",
            "batch no = 1856 / 2323, train loss = 4.042, lr = 0.012, \n",
            "batch no = 2088 / 2323, train loss = 4.531, lr = 0.012, \n",
            "batch no = 2320 / 2323, train loss = 4.446, lr = 0.012, \n",
            "Validation set perplexity : 98.419\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.813\n",
            "Training over\n",
            "Epoch 20/30\n",
            "batch no = 0 / 2323, train loss = 5.125, lr = 0.008, \n",
            "batch no = 232 / 2323, train loss = 4.198, lr = 0.008, \n",
            "batch no = 464 / 2323, train loss = 4.316, lr = 0.008, \n",
            "batch no = 696 / 2323, train loss = 4.289, lr = 0.008, \n",
            "batch no = 928 / 2323, train loss = 4.065, lr = 0.008, \n",
            "batch no = 1160 / 2323, train loss = 4.387, lr = 0.008, \n",
            "batch no = 1392 / 2323, train loss = 4.466, lr = 0.008, \n",
            "batch no = 1624 / 2323, train loss = 4.314, lr = 0.008, \n",
            "batch no = 1856 / 2323, train loss = 4.037, lr = 0.008, \n",
            "batch no = 2088 / 2323, train loss = 4.460, lr = 0.008, \n",
            "batch no = 2320 / 2323, train loss = 4.400, lr = 0.008, \n",
            "Validation set perplexity : 98.329\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.741\n",
            "Training over\n",
            "Epoch 21/30\n",
            "batch no = 0 / 2323, train loss = 5.134, lr = 0.005, \n",
            "batch no = 232 / 2323, train loss = 4.223, lr = 0.005, \n",
            "batch no = 464 / 2323, train loss = 4.292, lr = 0.005, \n",
            "batch no = 696 / 2323, train loss = 4.270, lr = 0.005, \n",
            "batch no = 928 / 2323, train loss = 4.110, lr = 0.005, \n",
            "batch no = 1160 / 2323, train loss = 4.391, lr = 0.005, \n",
            "batch no = 1392 / 2323, train loss = 4.440, lr = 0.005, \n",
            "batch no = 1624 / 2323, train loss = 4.297, lr = 0.005, \n",
            "batch no = 1856 / 2323, train loss = 4.042, lr = 0.005, \n",
            "batch no = 2088 / 2323, train loss = 4.487, lr = 0.005, \n",
            "batch no = 2320 / 2323, train loss = 4.412, lr = 0.005, \n",
            "Validation set perplexity : 98.275\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.685\n",
            "Training over\n",
            "Epoch 22/30\n",
            "batch no = 0 / 2323, train loss = 5.077, lr = 0.003, \n",
            "batch no = 232 / 2323, train loss = 4.265, lr = 0.003, \n",
            "batch no = 464 / 2323, train loss = 4.321, lr = 0.003, \n",
            "batch no = 696 / 2323, train loss = 4.330, lr = 0.003, \n",
            "batch no = 928 / 2323, train loss = 4.064, lr = 0.003, \n",
            "batch no = 1160 / 2323, train loss = 4.407, lr = 0.003, \n",
            "batch no = 1392 / 2323, train loss = 4.412, lr = 0.003, \n",
            "batch no = 1624 / 2323, train loss = 4.261, lr = 0.003, \n",
            "batch no = 1856 / 2323, train loss = 4.025, lr = 0.003, \n",
            "batch no = 2088 / 2323, train loss = 4.457, lr = 0.003, \n",
            "batch no = 2320 / 2323, train loss = 4.380, lr = 0.003, \n",
            "Validation set perplexity : 98.228\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.636\n",
            "Training over\n",
            "Epoch 23/30\n",
            "batch no = 0 / 2323, train loss = 5.122, lr = 0.002, \n",
            "batch no = 232 / 2323, train loss = 4.228, lr = 0.002, \n",
            "batch no = 464 / 2323, train loss = 4.337, lr = 0.002, \n",
            "batch no = 696 / 2323, train loss = 4.418, lr = 0.002, \n",
            "batch no = 928 / 2323, train loss = 4.089, lr = 0.002, \n",
            "batch no = 1160 / 2323, train loss = 4.394, lr = 0.002, \n",
            "batch no = 1392 / 2323, train loss = 4.428, lr = 0.002, \n",
            "batch no = 1624 / 2323, train loss = 4.317, lr = 0.002, \n",
            "batch no = 1856 / 2323, train loss = 4.036, lr = 0.002, \n",
            "batch no = 2088 / 2323, train loss = 4.446, lr = 0.002, \n",
            "batch no = 2320 / 2323, train loss = 4.364, lr = 0.002, \n",
            "Validation set perplexity : 98.210\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.609\n",
            "Training over\n",
            "Epoch 24/30\n",
            "batch no = 0 / 2323, train loss = 5.089, lr = 0.002, \n",
            "batch no = 232 / 2323, train loss = 4.258, lr = 0.002, \n",
            "batch no = 464 / 2323, train loss = 4.309, lr = 0.002, \n",
            "batch no = 696 / 2323, train loss = 4.316, lr = 0.002, \n",
            "batch no = 928 / 2323, train loss = 4.119, lr = 0.002, \n",
            "batch no = 1160 / 2323, train loss = 4.348, lr = 0.002, \n",
            "batch no = 1392 / 2323, train loss = 4.436, lr = 0.002, \n",
            "batch no = 1624 / 2323, train loss = 4.313, lr = 0.002, \n",
            "batch no = 1856 / 2323, train loss = 4.089, lr = 0.002, \n",
            "batch no = 2088 / 2323, train loss = 4.418, lr = 0.002, \n",
            "batch no = 2320 / 2323, train loss = 4.415, lr = 0.002, \n",
            "Validation set perplexity : 98.193\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.586\n",
            "Training over\n",
            "Epoch 25/30\n",
            "batch no = 0 / 2323, train loss = 5.203, lr = 0.001, \n",
            "batch no = 232 / 2323, train loss = 4.281, lr = 0.001, \n",
            "batch no = 464 / 2323, train loss = 4.309, lr = 0.001, \n",
            "batch no = 696 / 2323, train loss = 4.243, lr = 0.001, \n",
            "batch no = 928 / 2323, train loss = 4.158, lr = 0.001, \n",
            "batch no = 1160 / 2323, train loss = 4.393, lr = 0.001, \n",
            "batch no = 1392 / 2323, train loss = 4.485, lr = 0.001, \n",
            "batch no = 1624 / 2323, train loss = 4.215, lr = 0.001, \n",
            "batch no = 1856 / 2323, train loss = 4.057, lr = 0.001, \n",
            "batch no = 2088 / 2323, train loss = 4.359, lr = 0.001, \n",
            "batch no = 2320 / 2323, train loss = 4.437, lr = 0.001, \n",
            "Validation set perplexity : 98.186\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.572\n",
            "Training over\n",
            "Epoch 26/30\n",
            "batch no = 0 / 2323, train loss = 5.063, lr = 0.001, \n",
            "batch no = 232 / 2323, train loss = 4.321, lr = 0.001, \n",
            "batch no = 464 / 2323, train loss = 4.306, lr = 0.001, \n",
            "batch no = 696 / 2323, train loss = 4.347, lr = 0.001, \n",
            "batch no = 928 / 2323, train loss = 4.159, lr = 0.001, \n",
            "batch no = 1160 / 2323, train loss = 4.406, lr = 0.001, \n",
            "batch no = 1392 / 2323, train loss = 4.508, lr = 0.001, \n",
            "batch no = 1624 / 2323, train loss = 4.321, lr = 0.001, \n",
            "batch no = 1856 / 2323, train loss = 4.009, lr = 0.001, \n",
            "batch no = 2088 / 2323, train loss = 4.454, lr = 0.001, \n",
            "batch no = 2320 / 2323, train loss = 4.429, lr = 0.001, \n",
            "Validation set perplexity : 98.181\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.567\n",
            "Training over\n",
            "Epoch 27/30\n",
            "batch no = 0 / 2323, train loss = 5.149, lr = 0.000, \n",
            "batch no = 232 / 2323, train loss = 4.166, lr = 0.000, \n",
            "batch no = 464 / 2323, train loss = 4.370, lr = 0.000, \n",
            "batch no = 696 / 2323, train loss = 4.355, lr = 0.000, \n",
            "batch no = 928 / 2323, train loss = 4.100, lr = 0.000, \n",
            "batch no = 1160 / 2323, train loss = 4.421, lr = 0.000, \n",
            "batch no = 1392 / 2323, train loss = 4.437, lr = 0.000, \n",
            "batch no = 1624 / 2323, train loss = 4.290, lr = 0.000, \n",
            "batch no = 1856 / 2323, train loss = 3.980, lr = 0.000, \n",
            "batch no = 2088 / 2323, train loss = 4.404, lr = 0.000, \n",
            "batch no = 2320 / 2323, train loss = 4.364, lr = 0.000, \n",
            "Validation set perplexity : 98.176\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.564\n",
            "Training over\n",
            "Epoch 28/30\n",
            "batch no = 0 / 2323, train loss = 5.071, lr = 0.000, \n",
            "batch no = 232 / 2323, train loss = 4.265, lr = 0.000, \n",
            "batch no = 464 / 2323, train loss = 4.357, lr = 0.000, \n",
            "batch no = 696 / 2323, train loss = 4.275, lr = 0.000, \n",
            "batch no = 928 / 2323, train loss = 4.107, lr = 0.000, \n",
            "batch no = 1160 / 2323, train loss = 4.353, lr = 0.000, \n",
            "batch no = 1392 / 2323, train loss = 4.418, lr = 0.000, \n",
            "batch no = 1624 / 2323, train loss = 4.303, lr = 0.000, \n",
            "batch no = 1856 / 2323, train loss = 3.971, lr = 0.000, \n",
            "batch no = 2088 / 2323, train loss = 4.447, lr = 0.000, \n",
            "batch no = 2320 / 2323, train loss = 4.460, lr = 0.000, \n",
            "Validation set perplexity : 98.173\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.561\n",
            "Training over\n",
            "Epoch 29/30\n",
            "batch no = 0 / 2323, train loss = 5.099, lr = 0.000, \n",
            "batch no = 232 / 2323, train loss = 4.278, lr = 0.000, \n",
            "batch no = 464 / 2323, train loss = 4.298, lr = 0.000, \n",
            "batch no = 696 / 2323, train loss = 4.326, lr = 0.000, \n",
            "batch no = 928 / 2323, train loss = 4.220, lr = 0.000, \n",
            "batch no = 1160 / 2323, train loss = 4.285, lr = 0.000, \n",
            "batch no = 1392 / 2323, train loss = 4.487, lr = 0.000, \n",
            "batch no = 1624 / 2323, train loss = 4.285, lr = 0.000, \n",
            "batch no = 1856 / 2323, train loss = 4.037, lr = 0.000, \n",
            "batch no = 2088 / 2323, train loss = 4.445, lr = 0.000, \n",
            "batch no = 2320 / 2323, train loss = 4.406, lr = 0.000, \n",
            "Validation set perplexity : 98.170\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.559\n",
            "Training over\n",
            "Epoch 30/30\n",
            "batch no = 0 / 2323, train loss = 5.123, lr = 0.000, \n",
            "batch no = 232 / 2323, train loss = 4.326, lr = 0.000, \n",
            "batch no = 464 / 2323, train loss = 4.334, lr = 0.000, \n",
            "batch no = 696 / 2323, train loss = 4.339, lr = 0.000, \n",
            "batch no = 928 / 2323, train loss = 4.170, lr = 0.000, \n",
            "batch no = 1160 / 2323, train loss = 4.386, lr = 0.000, \n",
            "batch no = 1392 / 2323, train loss = 4.366, lr = 0.000, \n",
            "batch no = 1624 / 2323, train loss = 4.304, lr = 0.000, \n",
            "batch no = 1856 / 2323, train loss = 4.081, lr = 0.000, \n",
            "batch no = 2088 / 2323, train loss = 4.391, lr = 0.000, \n",
            "batch no = 2320 / 2323, train loss = 4.385, lr = 0.000, \n",
            "Validation set perplexity : 98.167\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.557\n",
            "Training over\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running GRU model without dropout\n",
        "\n",
        "model_gru = Model(words_len,200,2,0,0.1,'gru') #GRU model no dropout\n",
        "model_gru.to(device)\n",
        "num_epochs = 15\n",
        "lr = 0.42\n",
        "epoch_threshold = 4\n",
        "lr_factor = 1.2\n",
        "main((train,validation,test), num_epochs, epoch_threshold, lr, model_gru, \"GRU\",'gru',lr_factor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UDLA5psowJI",
        "outputId": "eb307b42-ae84-41fb-ac97-95d5f46b9947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "batch no = 0 / 2323, train loss = 9.221, lr = 0.420, \n",
            "batch no = 232 / 2323, train loss = 6.383, lr = 0.420, \n",
            "batch no = 464 / 2323, train loss = 5.997, lr = 0.420, \n",
            "batch no = 696 / 2323, train loss = 5.815, lr = 0.420, \n",
            "batch no = 928 / 2323, train loss = 5.337, lr = 0.420, \n",
            "batch no = 1160 / 2323, train loss = 5.439, lr = 0.420, \n",
            "batch no = 1392 / 2323, train loss = 5.553, lr = 0.420, \n",
            "batch no = 1624 / 2323, train loss = 5.220, lr = 0.420, \n",
            "batch no = 1856 / 2323, train loss = 4.975, lr = 0.420, \n",
            "batch no = 2088 / 2323, train loss = 5.517, lr = 0.420, \n",
            "batch no = 2320 / 2323, train loss = 5.372, lr = 0.420, \n",
            "Validation set perplexity : 191.152\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 189.938\n",
            "Training over\n",
            "Epoch 2/15\n",
            "batch no = 0 / 2323, train loss = 5.877, lr = 0.420, \n",
            "batch no = 232 / 2323, train loss = 4.986, lr = 0.420, \n",
            "batch no = 464 / 2323, train loss = 5.093, lr = 0.420, \n",
            "batch no = 696 / 2323, train loss = 5.035, lr = 0.420, \n",
            "batch no = 928 / 2323, train loss = 4.721, lr = 0.420, \n",
            "batch no = 1160 / 2323, train loss = 4.957, lr = 0.420, \n",
            "batch no = 1392 / 2323, train loss = 5.093, lr = 0.420, \n",
            "batch no = 1624 / 2323, train loss = 4.848, lr = 0.420, \n",
            "batch no = 1856 / 2323, train loss = 4.539, lr = 0.420, \n",
            "batch no = 2088 / 2323, train loss = 5.013, lr = 0.420, \n",
            "batch no = 2320 / 2323, train loss = 4.935, lr = 0.420, \n",
            "Validation set perplexity : 150.554\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 150.019\n",
            "Training over\n",
            "Epoch 3/15\n",
            "batch no = 0 / 2323, train loss = 5.509, lr = 0.420, \n",
            "batch no = 232 / 2323, train loss = 4.658, lr = 0.420, \n",
            "batch no = 464 / 2323, train loss = 4.701, lr = 0.420, \n",
            "batch no = 696 / 2323, train loss = 4.674, lr = 0.420, \n",
            "batch no = 928 / 2323, train loss = 4.392, lr = 0.420, \n",
            "batch no = 1160 / 2323, train loss = 4.669, lr = 0.420, \n",
            "batch no = 1392 / 2323, train loss = 4.817, lr = 0.420, \n",
            "batch no = 1624 / 2323, train loss = 4.636, lr = 0.420, \n",
            "batch no = 1856 / 2323, train loss = 4.296, lr = 0.420, \n",
            "batch no = 2088 / 2323, train loss = 4.705, lr = 0.420, \n",
            "batch no = 2320 / 2323, train loss = 4.632, lr = 0.420, \n",
            "Validation set perplexity : 143.585\n",
            "*************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru_dropout = Model(words_len,200,2,0.4,0.1,'gru') #GRU model 40% dropout\n",
        "model_gru_dropout.to(device)\n",
        "num_epochs = 30\n",
        "lr = 1\n",
        "lr_factor = 1.2\n",
        "epoch_threshold = 10\n",
        "main((train,validation,test), num_epochs, epoch_threshold, lr, model_gru_dropout,\"GRU_dropout\",'gru',lr_factor)"
      ],
      "metadata": {
        "id": "SVoYB8jVKtkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot perplexity vs epoch of test and train data from json files\n",
        "summary = {'LSTM Model' : [],\n",
        "           'LSTM Dropout Model': [],\n",
        "           'GRU Model': [],\n",
        "           'GRU Dropout Model':[]}\n",
        "with open('LSTM.json', mode='r', encoding='utf-8') as json_f:\n",
        "    results_dict = json.load(json_f)\n",
        "accuracies_regular_model = plt.figure()\n",
        "plt.plot(range(1, len(results_dict['train_perp']) + 1),\n",
        "             results_dict['train_perp'])\n",
        "plt.plot(range(1, len(results_dict['test_perp']) + 1),\n",
        "             results_dict['test_perp'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('Perplexity vs epoch for LSTM model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('perplexity')\n",
        "accuracies_regular_model.savefig('LSTM_perplexity_plot.png')\n",
        "plt.show()\n",
        "summary['LSTM Model'].append(min(results_dict['train_perp']))\n",
        "summary['LSTM Model'].append(min(results_dict['test_perp']))\n",
        "summary['LSTM Model'].append(min(results_dict['val_perp']))\n",
        "\n",
        "with open('LSTM_dropout.json', mode='r', encoding='utf-8') as json_f:\n",
        "    results_dict = json.load(json_f)\n",
        "accuracies_dropout_model = plt.figure()\n",
        "plt.plot(range(1, len(results_dict['train_perp']) + 1),\n",
        "             results_dict['train_perp'])\n",
        "plt.plot(range(1, len(results_dict['test_perp']) + 1),\n",
        "             results_dict['test_perp'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('Perplexity vs epoch for LSTM dropout model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('perplexity')\n",
        "accuracies_dropout_model.savefig('LSTM_dropout_perplexity_plot.png')\n",
        "plt.show()\n",
        "summary['LSTM Dropout Model'].append(min(results_dict['train_perp']))\n",
        "summary['LSTM Dropout Model'].append(min(results_dict['test_perp']))\n",
        "summary['LSTM Dropout Model'].append(min(results_dict['val_perp']))\n",
        "\n",
        "with open('GRU.json', mode='r', encoding='utf-8') as json_f:\n",
        "    results_dict = json.load(json_f)\n",
        "accuracies_wd_model = plt.figure()\n",
        "plt.plot(range(1, len(results_dict['train_perp']) + 1),\n",
        "             results_dict['train_perp'])\n",
        "plt.plot(range(1, len(results_dict['test_perp']) + 1),\n",
        "             results_dict['test_perp'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('Perplexity vs epoch for GRU model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('perplexity')\n",
        "accuracies_wd_model.savefig('GRU_perplexity_plot.png')\n",
        "plt.show()\n",
        "summary['GRU Model'].append(min(results_dict['train_perp']))\n",
        "summary['GRU Model'].append(min(results_dict['test_perp']))\n",
        "summary['GRU Model'].append(min(results_dict['val_perp']))\n",
        "\n",
        "with open('GRU_dropout.json', mode='r', encoding='utf-8') as json_f:\n",
        "    results_dict = json.load(json_f)\n",
        "accuracies_bn_model = plt.figure()\n",
        "plt.plot(range(1, len(results_dict['train_perp']) + 1),\n",
        "             results_dict['train_perp'])\n",
        "plt.plot(range(1, len(results_dict['test_perp']) + 1),\n",
        "             results_dict['test_perp'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('Perplexity vs epoch for GRU dropout model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('perplexity')\n",
        "accuracies_bn_model.savefig('GRU_dropout_perplexity_plot.png')\n",
        "plt.show()\n",
        "summary['GRU Dropout Model'].append(min(results_dict['train_perp']))\n",
        "summary['GRU Dropout Model'].append(min(results_dict['test_perp']))\n",
        "summary['GRU Dropout Model'].append(min(results_dict['val_perp']))"
      ],
      "metadata": {
        "id": "WDASKjPlp-4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"{:<20} {:<20} {:<20} {:<20}\".format('MODEL', 'VALIDATION', 'TRAIN', 'TEST'))\n",
        " \n",
        "#print max train and test accuracy for each model, summarize in a table\n",
        "for key, value in summary.items():\n",
        "    train,test,val = value\n",
        "    print(\"{:<20} {:<20} {:<20} {:<20}\".format(key, val, train, test))"
      ],
      "metadata": {
        "id": "yz9P4BOkr_Qj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}