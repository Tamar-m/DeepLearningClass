{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tamar-m/DeepLearningClass/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code adapted from: https://github.com/ahmetumutdurmus/zaremba/blob/master/main.py\n",
        "\n",
        "See also relevant paper: https://arxiv.org/pdf/1409.2329.pdf)"
      ],
      "metadata": {
        "id": "OpRQ96iMLemu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y6jzjADXuExQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2ce899-5cdf-4e62-f725-21799f9005a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at Google_drive; to attempt to forcibly remount, call drive.mount(\"Google_drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('Google_drive')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data must be google drive in following path: /Deep Learning Class/Ex2Data\n",
        "def data_load():\n",
        "    with open(\"/content/Google_drive/MyDrive/Deep Learning Class/Ex2Data/ptb.train.txt\") as f:\n",
        "        file = f.read()\n",
        "        trn = file[1:].split(' ')\n",
        "    with open(\"/content/Google_drive/MyDrive/Deep Learning Class/Ex2Data/ptb.valid.txt\") as f:\n",
        "        file = f.read()\n",
        "        vld = file[1:].split(' ')\n",
        "    with open(\"/content/Google_drive/MyDrive/Deep Learning Class/Ex2Data/ptb.test.txt\") as f:\n",
        "        file = f.read()\n",
        "        tst = file[1:].split(' ')\n",
        "    words = sorted(set(trn))\n",
        "    char2ind = {c: i for i, c in enumerate(words)}\n",
        "    trn = [char2ind[c] for c in trn]\n",
        "    vld = [char2ind[c] for c in vld]\n",
        "    tst = [char2ind[c] for c in tst]\n",
        "    return np.array(trn).reshape(-1, 1), np.array(vld).reshape(-1, 1), np.array(tst).reshape(-1, 1), len(words)\n",
        "\n",
        "#create minibatches of the data\n",
        "def minibatch(data, batch_size, seq_length):\n",
        "    data = torch.tensor(data, dtype = torch.int64)\n",
        "    num_batches = data.size(0)//batch_size\n",
        "    data = data[:num_batches*batch_size]\n",
        "    data=data.view(batch_size,-1)\n",
        "    dataset = []\n",
        "    for i in range(0,data.size(1)-1,seq_length):\n",
        "        seqlen=int(np.min([seq_length,data.size(1)-1-i]))\n",
        "        if seqlen<data.size(1)-1-i:\n",
        "            x=data[:,i:i+seqlen].transpose(1, 0)\n",
        "            y=data[:,i+1:i+seqlen+1].transpose(1, 0)\n",
        "            dataset.append((x, y))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "ezJmXB7IuHcD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding module\n",
        "class Embed(nn.Module): #embedding is efficient when we have a large number of input features\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.W = nn.Parameter(torch.Tensor(vocab_size, embed_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.W[x]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Embedding(vocab: {}, embedding: {})\".format(self.vocab_size, self.embed_size)\n",
        "\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "        self.b = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = torch.addmm(self.b, x.view(-1, x.size(2)), self.W.t())\n",
        "        return z\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"FC(input: {}, output: {})\".format(self.input_size, self.hidden_size)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, layer_num, dropout, winit, base):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layer_num = layer_num\n",
        "        self.winit = winit\n",
        "        self.embed = Embed(vocab_size, hidden_size)\n",
        "        if base == 'gru':\n",
        "          self.rnns = [nn.GRU(hidden_size, hidden_size) for i in range(layer_num)] #two GRU layers with 200 units per layer\n",
        "        else:\n",
        "          self.rnns = [nn.LSTM(hidden_size, hidden_size) for i in range(layer_num)] #two LSTM layers with 200 units per layer\n",
        "        self.rnns = nn.ModuleList(self.rnns)\n",
        "        self.fc = Linear(hidden_size, vocab_size) \n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.reset_parameters() #weights are reset uniformly when initializing the model\n",
        "        \n",
        "    def reset_parameters(self): #LSTM hidden layers weights initialized uniformly between -0.1 to 0.1\n",
        "        for param in self.parameters():\n",
        "            nn.init.uniform_(param, -self.winit, self.winit) \n",
        "            \n",
        "    def state_init(self, batch_size,base): #hidden states are initialized to zero, input of gru is h_0 and input of lstm is (h_0, c_0) \n",
        "        dev = next(self.parameters()).device\n",
        "        if base == 'gru':\n",
        "          states = [(torch.zeros(1, batch_size, layer.hidden_size, device = dev)) for layer in self.rnns]\n",
        "        else:\n",
        "          states = [(torch.zeros(1, batch_size, layer.hidden_size, device = dev), torch.zeros(1, batch_size, layer.hidden_size, device = dev)) for layer in self.rnns]\n",
        "        return states \n",
        "\n",
        "    # detach to complete truncated backpropagation through time (BPTT)\n",
        "    def detach(self, states, base):\n",
        "      if base == 'lstm':\n",
        "        return [(h.detach(), c.detach()) for (h,c) in states] # returns tensors that don't require a gradient\n",
        "      else:\n",
        "        return [h.detach() for h in states] #gru layers input of shape h_0\n",
        "\n",
        "    def forward(self, x, states):\n",
        "        x = self.embed(x)\n",
        "        x = self.dropout(x)\n",
        "        for i, rnn in enumerate(self.rnns):\n",
        "            x, states[i] = rnn(x, states[i])\n",
        "            x = self.dropout(x)\n",
        "        scores = self.fc(x)\n",
        "        return scores, states"
      ],
      "metadata": {
        "id": "w6j8oL5x5mbg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we define the loss (negative log likelihood) and perplexity"
      ],
      "metadata": {
        "id": "eLcQ38-oPJyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nll_loss(scores, y):\n",
        "    batch_size = y.size(1)\n",
        "    expscores = scores.exp()\n",
        "    probabilities = expscores / expscores.sum(1, keepdim = True)\n",
        "    answerprobs = probabilities[range(len(y.reshape(-1))), y.reshape(-1)]\n",
        "    return torch.mean(-torch.log(answerprobs) * batch_size)\n",
        "\n",
        "\n",
        "def perplexity(data,model,base):\n",
        "  with torch.no_grad():\n",
        "    losses = []\n",
        "    states = model.state_init(20,base)\n",
        "    for x,y in data:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      scores, states = model(x,states)\n",
        "      loss = nll_loss(scores, y)\n",
        "      losses.append(loss.data.item()/20)\n",
        "    return np.exp(np.mean(losses))"
      ],
      "metadata": {
        "id": "wxbdG19pYu8u"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(data, model, lr, max_norm,base):\n",
        "    trn, vld, tst = data\n",
        "    total_words = 0\n",
        "    states = model.state_init(20,base) # batch size of 20 according to article\n",
        "    model.train()\n",
        "    for i, (x, y) in enumerate(trn):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        total_words += x.numel()\n",
        "        model.zero_grad()\n",
        "        states = model.detach(states, base)\n",
        "        scores, states = model(x, states)\n",
        "        loss = nll_loss(scores, y)\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "            for param in model.parameters():\n",
        "                param -= lr * param.grad\n",
        "        if i % (len(trn)//10) == 0:\n",
        "            print(\"batch no = {:d} / {:d}, \".format(i, len(trn)) +\n",
        "                  \"train loss = {:.3f}, \".format(loss.item()/20) + #calculate loss\n",
        "                  \"lr = {:.3f}, \".format(lr))\n",
        "    model.eval()\n",
        "    val_perp = perplexity(vld, model,base)\n",
        "    print(\"Validation set perplexity : {:.3f}\".format(val_perp))\n",
        "    print(\"*************************************************\\n\")\n",
        "    return val_perp"
      ],
      "metadata": {
        "id": "i5Mze4n--UyX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(data,epochs,epoch_threshold,lr,model,model_name,base,lr_factor):\n",
        "  train, validation, test, word_len = data_load()\n",
        "  train = minibatch(train,20,20)\n",
        "  validation = minibatch(validation, 20, 20)\n",
        "  test = minibatch(test,20,20)\n",
        "  output_data = {\"model\": model_name,\n",
        "                 \"train_perp\": [],\n",
        "                 \"test_perp\":[],\n",
        "                 \"val_perp\":[]\n",
        "                 }\n",
        "  best_perp = np.Inf\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      if epoch > epoch_threshold: \n",
        "        lr = lr / lr_factor # after epoch threshold, learning rate decreased by a factor for each epoch\n",
        "      print(f'Epoch {epoch}/{epochs}')\n",
        "      val_perp = train_one_epoch((train,validation,test),model,lr,5,base)   \n",
        "      model.eval()   \n",
        "      test_perp = perplexity(test, model,base)\n",
        "      train_perp = perplexity(train, model,base)\n",
        "      output_data[\"train_perp\"].append(train_perp)\n",
        "      output_data[\"test_perp\"].append(test_perp)\n",
        "      output_data[\"val_perp\"].append(val_perp)\n",
        "      filename = model_name + '.json'\n",
        "      if test_perp < best_perp:\n",
        "        state = {\n",
        "                'model': model.state_dict(),\n",
        "                'train_perp': train_perp,\n",
        "                'test_perp': test_perp,\n",
        "                'epoch': epoch,\n",
        "                }\n",
        "        best_perp = test_perp\n",
        "        torch.save(state, model_name + '.pt')\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "          json.dump(output_data, f, indent=4)\n",
        "      print(\"Test set perplexity : {:.3f}\".format(test_perp))\n",
        "      print(\"Training over\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pOiIpRozUIDg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running LSTM model without dropout\n",
        "\n",
        "train, validation, test, words_len = data_load()\n",
        "train = minibatch(train,20,20)\n",
        "validation = minibatch(validation, 20, 20)\n",
        "test = minibatch(test,20,20)\n",
        "\n",
        "model_lstm = Model(words_len,200,2,0,0.1,'lstm') #LSTM model without dropout\n",
        "model_lstm.to(device)\n",
        "num_epochs = 16\n",
        "lr = 1\n",
        "epoch_threshold = 4\n",
        "main((train,validation,test), num_epochs, epoch_threshold, lr,model_lstm,\"LSTM\",'lstm',2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6E9gN-3aepr",
        "outputId": "d4241663-0771-4889-e3df-cf27aec10165"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "batch no = 0 / 2323, train loss = 9.208, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 6.330, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 5.951, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 5.708, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 5.313, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 5.399, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.509, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 5.219, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.925, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.486, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 5.360, lr = 1.000, \n",
            "Validation set perplexity : 186.534\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 184.924\n",
            "Training over\n",
            "Epoch 2/16\n",
            "batch no = 0 / 2323, train loss = 5.702, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 5.022, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 5.013, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.972, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.711, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.940, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.108, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.834, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.542, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.046, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 5.012, lr = 1.000, \n",
            "Validation set perplexity : 146.313\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 144.436\n",
            "Training over\n",
            "Epoch 3/16\n",
            "batch no = 0 / 2323, train loss = 5.455, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.634, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.727, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.692, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.467, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.769, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.856, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.626, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.385, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 4.780, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.741, lr = 1.000, \n",
            "Validation set perplexity : 134.602\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 132.927\n",
            "Training over\n",
            "Epoch 4/16\n",
            "batch no = 0 / 2323, train loss = 5.302, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.407, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.580, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.487, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.222, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.632, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.667, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.508, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.251, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 4.569, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.571, lr = 1.000, \n",
            "Validation set perplexity : 129.950\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 127.822\n",
            "Training over\n",
            "Epoch 5/16\n",
            "batch no = 0 / 2323, train loss = 5.158, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.222, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 4.300, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 4.249, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.009, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 4.336, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 4.379, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.295, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 3.972, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 4.228, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 4.263, lr = 0.500, \n",
            "Validation set perplexity : 120.273\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 117.651\n",
            "Training over\n",
            "Epoch 6/16\n",
            "batch no = 0 / 2323, train loss = 4.961, lr = 0.250, \n",
            "batch no = 232 / 2323, train loss = 3.987, lr = 0.250, \n",
            "batch no = 464 / 2323, train loss = 4.121, lr = 0.250, \n",
            "batch no = 696 / 2323, train loss = 4.056, lr = 0.250, \n",
            "batch no = 928 / 2323, train loss = 3.789, lr = 0.250, \n",
            "batch no = 1160 / 2323, train loss = 4.156, lr = 0.250, \n",
            "batch no = 1392 / 2323, train loss = 4.161, lr = 0.250, \n",
            "batch no = 1624 / 2323, train loss = 4.095, lr = 0.250, \n",
            "batch no = 1856 / 2323, train loss = 3.789, lr = 0.250, \n",
            "batch no = 2088 / 2323, train loss = 4.016, lr = 0.250, \n",
            "batch no = 2320 / 2323, train loss = 4.051, lr = 0.250, \n",
            "Validation set perplexity : 118.479\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 115.309\n",
            "Training over\n",
            "Epoch 7/16\n",
            "batch no = 0 / 2323, train loss = 4.882, lr = 0.125, \n",
            "batch no = 232 / 2323, train loss = 3.884, lr = 0.125, \n",
            "batch no = 464 / 2323, train loss = 4.002, lr = 0.125, \n",
            "batch no = 696 / 2323, train loss = 3.952, lr = 0.125, \n",
            "batch no = 928 / 2323, train loss = 3.677, lr = 0.125, \n",
            "batch no = 1160 / 2323, train loss = 4.074, lr = 0.125, \n",
            "batch no = 1392 / 2323, train loss = 4.032, lr = 0.125, \n",
            "batch no = 1624 / 2323, train loss = 3.994, lr = 0.125, \n",
            "batch no = 1856 / 2323, train loss = 3.665, lr = 0.125, \n",
            "batch no = 2088 / 2323, train loss = 3.880, lr = 0.125, \n",
            "batch no = 2320 / 2323, train loss = 3.912, lr = 0.125, \n",
            "Validation set perplexity : 118.121\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 114.876\n",
            "Training over\n",
            "Epoch 8/16\n",
            "batch no = 0 / 2323, train loss = 4.834, lr = 0.062, \n",
            "batch no = 232 / 2323, train loss = 3.822, lr = 0.062, \n",
            "batch no = 464 / 2323, train loss = 3.954, lr = 0.062, \n",
            "batch no = 696 / 2323, train loss = 3.901, lr = 0.062, \n",
            "batch no = 928 / 2323, train loss = 3.609, lr = 0.062, \n",
            "batch no = 1160 / 2323, train loss = 4.021, lr = 0.062, \n",
            "batch no = 1392 / 2323, train loss = 3.968, lr = 0.062, \n",
            "batch no = 1624 / 2323, train loss = 3.932, lr = 0.062, \n",
            "batch no = 1856 / 2323, train loss = 3.596, lr = 0.062, \n",
            "batch no = 2088 / 2323, train loss = 3.816, lr = 0.062, \n",
            "batch no = 2320 / 2323, train loss = 3.844, lr = 0.062, \n",
            "Validation set perplexity : 118.133\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 114.629\n",
            "Training over\n",
            "Epoch 9/16\n",
            "batch no = 0 / 2323, train loss = 4.806, lr = 0.031, \n",
            "batch no = 232 / 2323, train loss = 3.778, lr = 0.031, \n",
            "batch no = 464 / 2323, train loss = 3.929, lr = 0.031, \n",
            "batch no = 696 / 2323, train loss = 3.869, lr = 0.031, \n",
            "batch no = 928 / 2323, train loss = 3.576, lr = 0.031, \n",
            "batch no = 1160 / 2323, train loss = 3.990, lr = 0.031, \n",
            "batch no = 1392 / 2323, train loss = 3.932, lr = 0.031, \n",
            "batch no = 1624 / 2323, train loss = 3.895, lr = 0.031, \n",
            "batch no = 1856 / 2323, train loss = 3.554, lr = 0.031, \n",
            "batch no = 2088 / 2323, train loss = 3.773, lr = 0.031, \n",
            "batch no = 2320 / 2323, train loss = 3.809, lr = 0.031, \n",
            "Validation set perplexity : 118.149\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 114.399\n",
            "Training over\n",
            "Epoch 10/16\n",
            "batch no = 0 / 2323, train loss = 4.799, lr = 0.016, \n",
            "batch no = 232 / 2323, train loss = 3.750, lr = 0.016, \n",
            "batch no = 464 / 2323, train loss = 3.913, lr = 0.016, \n",
            "batch no = 696 / 2323, train loss = 3.852, lr = 0.016, \n",
            "batch no = 928 / 2323, train loss = 3.560, lr = 0.016, \n",
            "batch no = 1160 / 2323, train loss = 3.972, lr = 0.016, \n",
            "batch no = 1392 / 2323, train loss = 3.914, lr = 0.016, \n",
            "batch no = 1624 / 2323, train loss = 3.875, lr = 0.016, \n",
            "batch no = 1856 / 2323, train loss = 3.534, lr = 0.016, \n",
            "batch no = 2088 / 2323, train loss = 3.748, lr = 0.016, \n",
            "batch no = 2320 / 2323, train loss = 3.789, lr = 0.016, \n",
            "Validation set perplexity : 117.968\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 114.089\n",
            "Training over\n",
            "Epoch 11/16\n",
            "batch no = 0 / 2323, train loss = 4.794, lr = 0.008, \n",
            "batch no = 232 / 2323, train loss = 3.734, lr = 0.008, \n",
            "batch no = 464 / 2323, train loss = 3.902, lr = 0.008, \n",
            "batch no = 696 / 2323, train loss = 3.842, lr = 0.008, \n",
            "batch no = 928 / 2323, train loss = 3.550, lr = 0.008, \n",
            "batch no = 1160 / 2323, train loss = 3.961, lr = 0.008, \n",
            "batch no = 1392 / 2323, train loss = 3.902, lr = 0.008, \n",
            "batch no = 1624 / 2323, train loss = 3.864, lr = 0.008, \n",
            "batch no = 1856 / 2323, train loss = 3.526, lr = 0.008, \n",
            "batch no = 2088 / 2323, train loss = 3.733, lr = 0.008, \n",
            "batch no = 2320 / 2323, train loss = 3.779, lr = 0.008, \n",
            "Validation set perplexity : 117.665\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 113.758\n",
            "Training over\n",
            "Epoch 12/16\n",
            "batch no = 0 / 2323, train loss = 4.791, lr = 0.004, \n",
            "batch no = 232 / 2323, train loss = 3.725, lr = 0.004, \n",
            "batch no = 464 / 2323, train loss = 3.896, lr = 0.004, \n",
            "batch no = 696 / 2323, train loss = 3.836, lr = 0.004, \n",
            "batch no = 928 / 2323, train loss = 3.545, lr = 0.004, \n",
            "batch no = 1160 / 2323, train loss = 3.955, lr = 0.004, \n",
            "batch no = 1392 / 2323, train loss = 3.894, lr = 0.004, \n",
            "batch no = 1624 / 2323, train loss = 3.858, lr = 0.004, \n",
            "batch no = 1856 / 2323, train loss = 3.521, lr = 0.004, \n",
            "batch no = 2088 / 2323, train loss = 3.725, lr = 0.004, \n",
            "batch no = 2320 / 2323, train loss = 3.773, lr = 0.004, \n",
            "Validation set perplexity : 117.385\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 113.476\n",
            "Training over\n",
            "Epoch 13/16\n",
            "batch no = 0 / 2323, train loss = 4.789, lr = 0.002, \n",
            "batch no = 232 / 2323, train loss = 3.721, lr = 0.002, \n",
            "batch no = 464 / 2323, train loss = 3.892, lr = 0.002, \n",
            "batch no = 696 / 2323, train loss = 3.833, lr = 0.002, \n",
            "batch no = 928 / 2323, train loss = 3.543, lr = 0.002, \n",
            "batch no = 1160 / 2323, train loss = 3.953, lr = 0.002, \n",
            "batch no = 1392 / 2323, train loss = 3.889, lr = 0.002, \n",
            "batch no = 1624 / 2323, train loss = 3.854, lr = 0.002, \n",
            "batch no = 1856 / 2323, train loss = 3.519, lr = 0.002, \n",
            "batch no = 2088 / 2323, train loss = 3.720, lr = 0.002, \n",
            "batch no = 2320 / 2323, train loss = 3.769, lr = 0.002, \n",
            "Validation set perplexity : 117.192\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 113.281\n",
            "Training over\n",
            "Epoch 14/16\n",
            "batch no = 0 / 2323, train loss = 4.787, lr = 0.001, \n",
            "batch no = 232 / 2323, train loss = 3.719, lr = 0.001, \n",
            "batch no = 464 / 2323, train loss = 3.890, lr = 0.001, \n",
            "batch no = 696 / 2323, train loss = 3.831, lr = 0.001, \n",
            "batch no = 928 / 2323, train loss = 3.541, lr = 0.001, \n",
            "batch no = 1160 / 2323, train loss = 3.952, lr = 0.001, \n",
            "batch no = 1392 / 2323, train loss = 3.887, lr = 0.001, \n",
            "batch no = 1624 / 2323, train loss = 3.852, lr = 0.001, \n",
            "batch no = 1856 / 2323, train loss = 3.518, lr = 0.001, \n",
            "batch no = 2088 / 2323, train loss = 3.718, lr = 0.001, \n",
            "batch no = 2320 / 2323, train loss = 3.767, lr = 0.001, \n",
            "Validation set perplexity : 117.090\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 113.177\n",
            "Training over\n",
            "Epoch 15/16\n",
            "batch no = 0 / 2323, train loss = 4.787, lr = 0.000, \n",
            "batch no = 232 / 2323, train loss = 3.719, lr = 0.000, \n",
            "batch no = 464 / 2323, train loss = 3.889, lr = 0.000, \n",
            "batch no = 696 / 2323, train loss = 3.830, lr = 0.000, \n",
            "batch no = 928 / 2323, train loss = 3.540, lr = 0.000, \n",
            "batch no = 1160 / 2323, train loss = 3.951, lr = 0.000, \n",
            "batch no = 1392 / 2323, train loss = 3.886, lr = 0.000, \n",
            "batch no = 1624 / 2323, train loss = 3.851, lr = 0.000, \n",
            "batch no = 1856 / 2323, train loss = 3.517, lr = 0.000, \n",
            "batch no = 2088 / 2323, train loss = 3.717, lr = 0.000, \n",
            "batch no = 2320 / 2323, train loss = 3.766, lr = 0.000, \n",
            "Validation set perplexity : 117.039\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 113.127\n",
            "Training over\n",
            "Epoch 16/16\n",
            "batch no = 0 / 2323, train loss = 4.786, lr = 0.000, \n",
            "batch no = 232 / 2323, train loss = 3.718, lr = 0.000, \n",
            "batch no = 464 / 2323, train loss = 3.889, lr = 0.000, \n",
            "batch no = 696 / 2323, train loss = 3.829, lr = 0.000, \n",
            "batch no = 928 / 2323, train loss = 3.540, lr = 0.000, \n",
            "batch no = 1160 / 2323, train loss = 3.951, lr = 0.000, \n",
            "batch no = 1392 / 2323, train loss = 3.885, lr = 0.000, \n",
            "batch no = 1624 / 2323, train loss = 3.850, lr = 0.000, \n",
            "batch no = 1856 / 2323, train loss = 3.517, lr = 0.000, \n",
            "batch no = 2088 / 2323, train loss = 3.717, lr = 0.000, \n",
            "batch no = 2320 / 2323, train loss = 3.765, lr = 0.000, \n",
            "Validation set perplexity : 117.014\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 113.103\n",
            "Training over\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running LSTM model with 30% dropout\n",
        "train, validation, test, words_len = data_load()\n",
        "model_lstm_dropout = Model(words_len,200,2,0.3,0.1,'lstm') #LSTM model with 30% dropout\n",
        "model_lstm_dropout.to(device)\n",
        "num_epochs = 30\n",
        "lr = 1\n",
        "lr_factor = 1.5\n",
        "epoch_threshold = 8\n",
        "main((train,validation,test), num_epochs, epoch_threshold, lr, model_lstm_dropout,\"LSTM_dropout\",'lstm',lr_factor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGAinqsfGp_T",
        "outputId": "bb9c39fd-23f6-4414-a719-d6afd36a9bd1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "batch no = 0 / 2323, train loss = 9.217, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 6.367, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 6.074, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 5.824, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 5.483, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 5.577, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.669, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 5.392, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 5.104, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.639, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 5.548, lr = 1.000, \n",
            "Validation set perplexity : 197.622\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 194.153\n",
            "Training over\n",
            "Epoch 2/30\n",
            "batch no = 0 / 2323, train loss = 5.872, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 5.197, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 5.193, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 5.238, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.952, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 5.149, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.301, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 5.090, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.793, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.354, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 5.277, lr = 1.000, \n",
            "Validation set perplexity : 156.828\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 154.581\n",
            "Training over\n",
            "Epoch 3/30\n",
            "batch no = 0 / 2323, train loss = 5.682, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.988, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 5.088, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 5.025, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.741, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 5.033, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.179, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.877, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.659, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.175, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 5.117, lr = 1.000, \n",
            "Validation set perplexity : 140.740\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 138.436\n",
            "Training over\n",
            "Epoch 4/30\n",
            "batch no = 0 / 2323, train loss = 5.563, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.838, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.910, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.838, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.676, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.839, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 5.075, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.858, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.589, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.182, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.992, lr = 1.000, \n",
            "Validation set perplexity : 131.538\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 128.697\n",
            "Training over\n",
            "Epoch 5/30\n",
            "batch no = 0 / 2323, train loss = 5.491, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.826, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.857, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.752, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.489, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.853, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.997, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.763, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.520, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 5.004, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.928, lr = 1.000, \n",
            "Validation set perplexity : 126.495\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 124.551\n",
            "Training over\n",
            "Epoch 6/30\n",
            "batch no = 0 / 2323, train loss = 5.450, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.770, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.735, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.725, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.609, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.792, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.904, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.771, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.479, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 4.940, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.896, lr = 1.000, \n",
            "Validation set perplexity : 122.289\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 120.216\n",
            "Training over\n",
            "Epoch 7/30\n",
            "batch no = 0 / 2323, train loss = 5.382, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.617, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.730, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.692, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.427, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.750, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.967, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.671, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.513, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 4.956, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.788, lr = 1.000, \n",
            "Validation set perplexity : 119.755\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 117.031\n",
            "Training over\n",
            "Epoch 8/30\n",
            "batch no = 0 / 2323, train loss = 5.409, lr = 1.000, \n",
            "batch no = 232 / 2323, train loss = 4.619, lr = 1.000, \n",
            "batch no = 464 / 2323, train loss = 4.723, lr = 1.000, \n",
            "batch no = 696 / 2323, train loss = 4.647, lr = 1.000, \n",
            "batch no = 928 / 2323, train loss = 4.493, lr = 1.000, \n",
            "batch no = 1160 / 2323, train loss = 4.690, lr = 1.000, \n",
            "batch no = 1392 / 2323, train loss = 4.908, lr = 1.000, \n",
            "batch no = 1624 / 2323, train loss = 4.688, lr = 1.000, \n",
            "batch no = 1856 / 2323, train loss = 4.466, lr = 1.000, \n",
            "batch no = 2088 / 2323, train loss = 4.913, lr = 1.000, \n",
            "batch no = 2320 / 2323, train loss = 4.771, lr = 1.000, \n",
            "Validation set perplexity : 118.570\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 115.623\n",
            "Training over\n",
            "Epoch 9/30\n",
            "batch no = 0 / 2323, train loss = 5.303, lr = 0.667, \n",
            "batch no = 232 / 2323, train loss = 4.635, lr = 0.667, \n",
            "batch no = 464 / 2323, train loss = 4.682, lr = 0.667, \n",
            "batch no = 696 / 2323, train loss = 4.562, lr = 0.667, \n",
            "batch no = 928 / 2323, train loss = 4.478, lr = 0.667, \n",
            "batch no = 1160 / 2323, train loss = 4.599, lr = 0.667, \n",
            "batch no = 1392 / 2323, train loss = 4.694, lr = 0.667, \n",
            "batch no = 1624 / 2323, train loss = 4.593, lr = 0.667, \n",
            "batch no = 1856 / 2323, train loss = 4.371, lr = 0.667, \n",
            "batch no = 2088 / 2323, train loss = 4.702, lr = 0.667, \n",
            "batch no = 2320 / 2323, train loss = 4.645, lr = 0.667, \n",
            "Validation set perplexity : 110.952\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 108.557\n",
            "Training over\n",
            "Epoch 10/30\n",
            "batch no = 0 / 2323, train loss = 5.334, lr = 0.444, \n",
            "batch no = 232 / 2323, train loss = 4.541, lr = 0.444, \n",
            "batch no = 464 / 2323, train loss = 4.563, lr = 0.444, \n",
            "batch no = 696 / 2323, train loss = 4.536, lr = 0.444, \n",
            "batch no = 928 / 2323, train loss = 4.322, lr = 0.444, \n",
            "batch no = 1160 / 2323, train loss = 4.592, lr = 0.444, \n",
            "batch no = 1392 / 2323, train loss = 4.605, lr = 0.444, \n",
            "batch no = 1624 / 2323, train loss = 4.481, lr = 0.444, \n",
            "batch no = 1856 / 2323, train loss = 4.168, lr = 0.444, \n",
            "batch no = 2088 / 2323, train loss = 4.602, lr = 0.444, \n",
            "batch no = 2320 / 2323, train loss = 4.657, lr = 0.444, \n",
            "Validation set perplexity : 106.299\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 103.580\n",
            "Training over\n",
            "Epoch 11/30\n",
            "batch no = 0 / 2323, train loss = 5.258, lr = 0.296, \n",
            "batch no = 232 / 2323, train loss = 4.488, lr = 0.296, \n",
            "batch no = 464 / 2323, train loss = 4.496, lr = 0.296, \n",
            "batch no = 696 / 2323, train loss = 4.390, lr = 0.296, \n",
            "batch no = 928 / 2323, train loss = 4.306, lr = 0.296, \n",
            "batch no = 1160 / 2323, train loss = 4.494, lr = 0.296, \n",
            "batch no = 1392 / 2323, train loss = 4.571, lr = 0.296, \n",
            "batch no = 1624 / 2323, train loss = 4.410, lr = 0.296, \n",
            "batch no = 1856 / 2323, train loss = 4.154, lr = 0.296, \n",
            "batch no = 2088 / 2323, train loss = 4.491, lr = 0.296, \n",
            "batch no = 2320 / 2323, train loss = 4.524, lr = 0.296, \n",
            "Validation set perplexity : 103.937\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 100.804\n",
            "Training over\n",
            "Epoch 12/30\n",
            "batch no = 0 / 2323, train loss = 5.192, lr = 0.198, \n",
            "batch no = 232 / 2323, train loss = 4.369, lr = 0.198, \n",
            "batch no = 464 / 2323, train loss = 4.450, lr = 0.198, \n",
            "batch no = 696 / 2323, train loss = 4.388, lr = 0.198, \n",
            "batch no = 928 / 2323, train loss = 4.244, lr = 0.198, \n",
            "batch no = 1160 / 2323, train loss = 4.452, lr = 0.198, \n",
            "batch no = 1392 / 2323, train loss = 4.540, lr = 0.198, \n",
            "batch no = 1624 / 2323, train loss = 4.411, lr = 0.198, \n",
            "batch no = 1856 / 2323, train loss = 4.121, lr = 0.198, \n",
            "batch no = 2088 / 2323, train loss = 4.497, lr = 0.198, \n",
            "batch no = 2320 / 2323, train loss = 4.529, lr = 0.198, \n",
            "Validation set perplexity : 101.875\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 98.700\n",
            "Training over\n",
            "Epoch 13/30\n",
            "batch no = 0 / 2323, train loss = 5.147, lr = 0.132, \n",
            "batch no = 232 / 2323, train loss = 4.378, lr = 0.132, \n",
            "batch no = 464 / 2323, train loss = 4.318, lr = 0.132, \n",
            "batch no = 696 / 2323, train loss = 4.344, lr = 0.132, \n",
            "batch no = 928 / 2323, train loss = 4.254, lr = 0.132, \n",
            "batch no = 1160 / 2323, train loss = 4.448, lr = 0.132, \n",
            "batch no = 1392 / 2323, train loss = 4.530, lr = 0.132, \n",
            "batch no = 1624 / 2323, train loss = 4.315, lr = 0.132, \n",
            "batch no = 1856 / 2323, train loss = 4.067, lr = 0.132, \n",
            "batch no = 2088 / 2323, train loss = 4.472, lr = 0.132, \n",
            "batch no = 2320 / 2323, train loss = 4.443, lr = 0.132, \n",
            "Validation set perplexity : 100.646\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 97.398\n",
            "Training over\n",
            "Epoch 14/30\n",
            "batch no = 0 / 2323, train loss = 5.095, lr = 0.088, \n",
            "batch no = 232 / 2323, train loss = 4.295, lr = 0.088, \n",
            "batch no = 464 / 2323, train loss = 4.361, lr = 0.088, \n",
            "batch no = 696 / 2323, train loss = 4.373, lr = 0.088, \n",
            "batch no = 928 / 2323, train loss = 4.190, lr = 0.088, \n",
            "batch no = 1160 / 2323, train loss = 4.319, lr = 0.088, \n",
            "batch no = 1392 / 2323, train loss = 4.486, lr = 0.088, \n",
            "batch no = 1624 / 2323, train loss = 4.389, lr = 0.088, \n",
            "batch no = 1856 / 2323, train loss = 4.088, lr = 0.088, \n",
            "batch no = 2088 / 2323, train loss = 4.449, lr = 0.088, \n",
            "batch no = 2320 / 2323, train loss = 4.394, lr = 0.088, \n",
            "Validation set perplexity : 99.701\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.335\n",
            "Training over\n",
            "Epoch 15/30\n",
            "batch no = 0 / 2323, train loss = 5.145, lr = 0.059, \n",
            "batch no = 232 / 2323, train loss = 4.308, lr = 0.059, \n",
            "batch no = 464 / 2323, train loss = 4.359, lr = 0.059, \n",
            "batch no = 696 / 2323, train loss = 4.321, lr = 0.059, \n",
            "batch no = 928 / 2323, train loss = 4.131, lr = 0.059, \n",
            "batch no = 1160 / 2323, train loss = 4.413, lr = 0.059, \n",
            "batch no = 1392 / 2323, train loss = 4.431, lr = 0.059, \n",
            "batch no = 1624 / 2323, train loss = 4.341, lr = 0.059, \n",
            "batch no = 1856 / 2323, train loss = 3.974, lr = 0.059, \n",
            "batch no = 2088 / 2323, train loss = 4.412, lr = 0.059, \n",
            "batch no = 2320 / 2323, train loss = 4.439, lr = 0.059, \n",
            "Validation set perplexity : 99.125\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.692\n",
            "Training over\n",
            "Epoch 16/30\n",
            "batch no = 0 / 2323, train loss = 5.106, lr = 0.039, \n",
            "batch no = 232 / 2323, train loss = 4.397, lr = 0.039, \n",
            "batch no = 464 / 2323, train loss = 4.354, lr = 0.039, \n",
            "batch no = 696 / 2323, train loss = 4.273, lr = 0.039, \n",
            "batch no = 928 / 2323, train loss = 4.217, lr = 0.039, \n",
            "batch no = 1160 / 2323, train loss = 4.405, lr = 0.039, \n",
            "batch no = 1392 / 2323, train loss = 4.463, lr = 0.039, \n",
            "batch no = 1624 / 2323, train loss = 4.318, lr = 0.039, \n",
            "batch no = 1856 / 2323, train loss = 4.039, lr = 0.039, \n",
            "batch no = 2088 / 2323, train loss = 4.471, lr = 0.039, \n",
            "batch no = 2320 / 2323, train loss = 4.451, lr = 0.039, \n",
            "Validation set perplexity : 98.849\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.359\n",
            "Training over\n",
            "Epoch 17/30\n",
            "batch no = 0 / 2323, train loss = 5.150, lr = 0.026, \n",
            "batch no = 232 / 2323, train loss = 4.260, lr = 0.026, \n",
            "batch no = 464 / 2323, train loss = 4.364, lr = 0.026, \n",
            "batch no = 696 / 2323, train loss = 4.335, lr = 0.026, \n",
            "batch no = 928 / 2323, train loss = 4.131, lr = 0.026, \n",
            "batch no = 1160 / 2323, train loss = 4.421, lr = 0.026, \n",
            "batch no = 1392 / 2323, train loss = 4.500, lr = 0.026, \n",
            "batch no = 1624 / 2323, train loss = 4.312, lr = 0.026, \n",
            "batch no = 1856 / 2323, train loss = 3.987, lr = 0.026, \n",
            "batch no = 2088 / 2323, train loss = 4.423, lr = 0.026, \n",
            "batch no = 2320 / 2323, train loss = 4.401, lr = 0.026, \n",
            "Validation set perplexity : 98.628\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.082\n",
            "Training over\n",
            "Epoch 18/30\n",
            "batch no = 0 / 2323, train loss = 5.097, lr = 0.017, \n",
            "batch no = 232 / 2323, train loss = 4.274, lr = 0.017, \n",
            "batch no = 464 / 2323, train loss = 4.316, lr = 0.017, \n",
            "batch no = 696 / 2323, train loss = 4.301, lr = 0.017, \n",
            "batch no = 928 / 2323, train loss = 4.110, lr = 0.017, \n",
            "batch no = 1160 / 2323, train loss = 4.325, lr = 0.017, \n",
            "batch no = 1392 / 2323, train loss = 4.494, lr = 0.017, \n",
            "batch no = 1624 / 2323, train loss = 4.290, lr = 0.017, \n",
            "batch no = 1856 / 2323, train loss = 4.058, lr = 0.017, \n",
            "batch no = 2088 / 2323, train loss = 4.414, lr = 0.017, \n",
            "batch no = 2320 / 2323, train loss = 4.406, lr = 0.017, \n",
            "Validation set perplexity : 98.479\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.932\n",
            "Training over\n",
            "Epoch 19/30\n",
            "batch no = 0 / 2323, train loss = 5.111, lr = 0.012, \n",
            "batch no = 232 / 2323, train loss = 4.244, lr = 0.012, \n",
            "batch no = 464 / 2323, train loss = 4.319, lr = 0.012, \n",
            "batch no = 696 / 2323, train loss = 4.360, lr = 0.012, \n",
            "batch no = 928 / 2323, train loss = 4.194, lr = 0.012, \n",
            "batch no = 1160 / 2323, train loss = 4.486, lr = 0.012, \n",
            "batch no = 1392 / 2323, train loss = 4.460, lr = 0.012, \n",
            "batch no = 1624 / 2323, train loss = 4.284, lr = 0.012, \n",
            "batch no = 1856 / 2323, train loss = 4.042, lr = 0.012, \n",
            "batch no = 2088 / 2323, train loss = 4.531, lr = 0.012, \n",
            "batch no = 2320 / 2323, train loss = 4.446, lr = 0.012, \n",
            "Validation set perplexity : 98.419\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.813\n",
            "Training over\n",
            "Epoch 20/30\n",
            "batch no = 0 / 2323, train loss = 5.125, lr = 0.008, \n",
            "batch no = 232 / 2323, train loss = 4.198, lr = 0.008, \n",
            "batch no = 464 / 2323, train loss = 4.316, lr = 0.008, \n",
            "batch no = 696 / 2323, train loss = 4.289, lr = 0.008, \n",
            "batch no = 928 / 2323, train loss = 4.065, lr = 0.008, \n",
            "batch no = 1160 / 2323, train loss = 4.387, lr = 0.008, \n",
            "batch no = 1392 / 2323, train loss = 4.466, lr = 0.008, \n",
            "batch no = 1624 / 2323, train loss = 4.314, lr = 0.008, \n",
            "batch no = 1856 / 2323, train loss = 4.037, lr = 0.008, \n",
            "batch no = 2088 / 2323, train loss = 4.460, lr = 0.008, \n",
            "batch no = 2320 / 2323, train loss = 4.400, lr = 0.008, \n",
            "Validation set perplexity : 98.329\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.741\n",
            "Training over\n",
            "Epoch 21/30\n",
            "batch no = 0 / 2323, train loss = 5.134, lr = 0.005, \n",
            "batch no = 232 / 2323, train loss = 4.223, lr = 0.005, \n",
            "batch no = 464 / 2323, train loss = 4.292, lr = 0.005, \n",
            "batch no = 696 / 2323, train loss = 4.270, lr = 0.005, \n",
            "batch no = 928 / 2323, train loss = 4.110, lr = 0.005, \n",
            "batch no = 1160 / 2323, train loss = 4.391, lr = 0.005, \n",
            "batch no = 1392 / 2323, train loss = 4.440, lr = 0.005, \n",
            "batch no = 1624 / 2323, train loss = 4.297, lr = 0.005, \n",
            "batch no = 1856 / 2323, train loss = 4.042, lr = 0.005, \n",
            "batch no = 2088 / 2323, train loss = 4.487, lr = 0.005, \n",
            "batch no = 2320 / 2323, train loss = 4.412, lr = 0.005, \n",
            "Validation set perplexity : 98.275\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.685\n",
            "Training over\n",
            "Epoch 22/30\n",
            "batch no = 0 / 2323, train loss = 5.077, lr = 0.003, \n",
            "batch no = 232 / 2323, train loss = 4.265, lr = 0.003, \n",
            "batch no = 464 / 2323, train loss = 4.321, lr = 0.003, \n",
            "batch no = 696 / 2323, train loss = 4.330, lr = 0.003, \n",
            "batch no = 928 / 2323, train loss = 4.064, lr = 0.003, \n",
            "batch no = 1160 / 2323, train loss = 4.407, lr = 0.003, \n",
            "batch no = 1392 / 2323, train loss = 4.412, lr = 0.003, \n",
            "batch no = 1624 / 2323, train loss = 4.261, lr = 0.003, \n",
            "batch no = 1856 / 2323, train loss = 4.025, lr = 0.003, \n",
            "batch no = 2088 / 2323, train loss = 4.457, lr = 0.003, \n",
            "batch no = 2320 / 2323, train loss = 4.380, lr = 0.003, \n",
            "Validation set perplexity : 98.228\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.636\n",
            "Training over\n",
            "Epoch 23/30\n",
            "batch no = 0 / 2323, train loss = 5.122, lr = 0.002, \n",
            "batch no = 232 / 2323, train loss = 4.228, lr = 0.002, \n",
            "batch no = 464 / 2323, train loss = 4.337, lr = 0.002, \n",
            "batch no = 696 / 2323, train loss = 4.418, lr = 0.002, \n",
            "batch no = 928 / 2323, train loss = 4.089, lr = 0.002, \n",
            "batch no = 1160 / 2323, train loss = 4.394, lr = 0.002, \n",
            "batch no = 1392 / 2323, train loss = 4.428, lr = 0.002, \n",
            "batch no = 1624 / 2323, train loss = 4.317, lr = 0.002, \n",
            "batch no = 1856 / 2323, train loss = 4.036, lr = 0.002, \n",
            "batch no = 2088 / 2323, train loss = 4.446, lr = 0.002, \n",
            "batch no = 2320 / 2323, train loss = 4.364, lr = 0.002, \n",
            "Validation set perplexity : 98.210\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.609\n",
            "Training over\n",
            "Epoch 24/30\n",
            "batch no = 0 / 2323, train loss = 5.089, lr = 0.002, \n",
            "batch no = 232 / 2323, train loss = 4.258, lr = 0.002, \n",
            "batch no = 464 / 2323, train loss = 4.309, lr = 0.002, \n",
            "batch no = 696 / 2323, train loss = 4.316, lr = 0.002, \n",
            "batch no = 928 / 2323, train loss = 4.119, lr = 0.002, \n",
            "batch no = 1160 / 2323, train loss = 4.348, lr = 0.002, \n",
            "batch no = 1392 / 2323, train loss = 4.436, lr = 0.002, \n",
            "batch no = 1624 / 2323, train loss = 4.313, lr = 0.002, \n",
            "batch no = 1856 / 2323, train loss = 4.089, lr = 0.002, \n",
            "batch no = 2088 / 2323, train loss = 4.418, lr = 0.002, \n",
            "batch no = 2320 / 2323, train loss = 4.415, lr = 0.002, \n",
            "Validation set perplexity : 98.193\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.586\n",
            "Training over\n",
            "Epoch 25/30\n",
            "batch no = 0 / 2323, train loss = 5.203, lr = 0.001, \n",
            "batch no = 232 / 2323, train loss = 4.281, lr = 0.001, \n",
            "batch no = 464 / 2323, train loss = 4.309, lr = 0.001, \n",
            "batch no = 696 / 2323, train loss = 4.243, lr = 0.001, \n",
            "batch no = 928 / 2323, train loss = 4.158, lr = 0.001, \n",
            "batch no = 1160 / 2323, train loss = 4.393, lr = 0.001, \n",
            "batch no = 1392 / 2323, train loss = 4.485, lr = 0.001, \n",
            "batch no = 1624 / 2323, train loss = 4.215, lr = 0.001, \n",
            "batch no = 1856 / 2323, train loss = 4.057, lr = 0.001, \n",
            "batch no = 2088 / 2323, train loss = 4.359, lr = 0.001, \n",
            "batch no = 2320 / 2323, train loss = 4.437, lr = 0.001, \n",
            "Validation set perplexity : 98.186\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.572\n",
            "Training over\n",
            "Epoch 26/30\n",
            "batch no = 0 / 2323, train loss = 5.063, lr = 0.001, \n",
            "batch no = 232 / 2323, train loss = 4.321, lr = 0.001, \n",
            "batch no = 464 / 2323, train loss = 4.306, lr = 0.001, \n",
            "batch no = 696 / 2323, train loss = 4.347, lr = 0.001, \n",
            "batch no = 928 / 2323, train loss = 4.159, lr = 0.001, \n",
            "batch no = 1160 / 2323, train loss = 4.406, lr = 0.001, \n",
            "batch no = 1392 / 2323, train loss = 4.508, lr = 0.001, \n",
            "batch no = 1624 / 2323, train loss = 4.321, lr = 0.001, \n",
            "batch no = 1856 / 2323, train loss = 4.009, lr = 0.001, \n",
            "batch no = 2088 / 2323, train loss = 4.454, lr = 0.001, \n",
            "batch no = 2320 / 2323, train loss = 4.429, lr = 0.001, \n",
            "Validation set perplexity : 98.181\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.567\n",
            "Training over\n",
            "Epoch 27/30\n",
            "batch no = 0 / 2323, train loss = 5.149, lr = 0.000, \n",
            "batch no = 232 / 2323, train loss = 4.166, lr = 0.000, \n",
            "batch no = 464 / 2323, train loss = 4.370, lr = 0.000, \n",
            "batch no = 696 / 2323, train loss = 4.355, lr = 0.000, \n",
            "batch no = 928 / 2323, train loss = 4.100, lr = 0.000, \n",
            "batch no = 1160 / 2323, train loss = 4.421, lr = 0.000, \n",
            "batch no = 1392 / 2323, train loss = 4.437, lr = 0.000, \n",
            "batch no = 1624 / 2323, train loss = 4.290, lr = 0.000, \n",
            "batch no = 1856 / 2323, train loss = 3.980, lr = 0.000, \n",
            "batch no = 2088 / 2323, train loss = 4.404, lr = 0.000, \n",
            "batch no = 2320 / 2323, train loss = 4.364, lr = 0.000, \n",
            "Validation set perplexity : 98.176\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.564\n",
            "Training over\n",
            "Epoch 28/30\n",
            "batch no = 0 / 2323, train loss = 5.071, lr = 0.000, \n",
            "batch no = 232 / 2323, train loss = 4.265, lr = 0.000, \n",
            "batch no = 464 / 2323, train loss = 4.357, lr = 0.000, \n",
            "batch no = 696 / 2323, train loss = 4.275, lr = 0.000, \n",
            "batch no = 928 / 2323, train loss = 4.107, lr = 0.000, \n",
            "batch no = 1160 / 2323, train loss = 4.353, lr = 0.000, \n",
            "batch no = 1392 / 2323, train loss = 4.418, lr = 0.000, \n",
            "batch no = 1624 / 2323, train loss = 4.303, lr = 0.000, \n",
            "batch no = 1856 / 2323, train loss = 3.971, lr = 0.000, \n",
            "batch no = 2088 / 2323, train loss = 4.447, lr = 0.000, \n",
            "batch no = 2320 / 2323, train loss = 4.460, lr = 0.000, \n",
            "Validation set perplexity : 98.173\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.561\n",
            "Training over\n",
            "Epoch 29/30\n",
            "batch no = 0 / 2323, train loss = 5.099, lr = 0.000, \n",
            "batch no = 232 / 2323, train loss = 4.278, lr = 0.000, \n",
            "batch no = 464 / 2323, train loss = 4.298, lr = 0.000, \n",
            "batch no = 696 / 2323, train loss = 4.326, lr = 0.000, \n",
            "batch no = 928 / 2323, train loss = 4.220, lr = 0.000, \n",
            "batch no = 1160 / 2323, train loss = 4.285, lr = 0.000, \n",
            "batch no = 1392 / 2323, train loss = 4.487, lr = 0.000, \n",
            "batch no = 1624 / 2323, train loss = 4.285, lr = 0.000, \n",
            "batch no = 1856 / 2323, train loss = 4.037, lr = 0.000, \n",
            "batch no = 2088 / 2323, train loss = 4.445, lr = 0.000, \n",
            "batch no = 2320 / 2323, train loss = 4.406, lr = 0.000, \n",
            "Validation set perplexity : 98.170\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.559\n",
            "Training over\n",
            "Epoch 30/30\n",
            "batch no = 0 / 2323, train loss = 5.123, lr = 0.000, \n",
            "batch no = 232 / 2323, train loss = 4.326, lr = 0.000, \n",
            "batch no = 464 / 2323, train loss = 4.334, lr = 0.000, \n",
            "batch no = 696 / 2323, train loss = 4.339, lr = 0.000, \n",
            "batch no = 928 / 2323, train loss = 4.170, lr = 0.000, \n",
            "batch no = 1160 / 2323, train loss = 4.386, lr = 0.000, \n",
            "batch no = 1392 / 2323, train loss = 4.366, lr = 0.000, \n",
            "batch no = 1624 / 2323, train loss = 4.304, lr = 0.000, \n",
            "batch no = 1856 / 2323, train loss = 4.081, lr = 0.000, \n",
            "batch no = 2088 / 2323, train loss = 4.391, lr = 0.000, \n",
            "batch no = 2320 / 2323, train loss = 4.385, lr = 0.000, \n",
            "Validation set perplexity : 98.167\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 94.557\n",
            "Training over\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running GRU model without dropout\n",
        "\n",
        "model_gru = Model(words_len,200,2,0,0.1,'gru') #GRU model no dropout\n",
        "model_gru.to(device)\n",
        "num_epochs = 15\n",
        "lr = 0.42\n",
        "epoch_threshold = 4\n",
        "lr_factor = 1.2\n",
        "main((train,validation,test), num_epochs, epoch_threshold, lr, model_gru, \"GRU\",'gru',lr_factor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UDLA5psowJI",
        "outputId": "eb307b42-ae84-41fb-ac97-95d5f46b9947"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "batch no = 0 / 2323, train loss = 9.221, lr = 0.420, \n",
            "batch no = 232 / 2323, train loss = 6.383, lr = 0.420, \n",
            "batch no = 464 / 2323, train loss = 5.997, lr = 0.420, \n",
            "batch no = 696 / 2323, train loss = 5.815, lr = 0.420, \n",
            "batch no = 928 / 2323, train loss = 5.337, lr = 0.420, \n",
            "batch no = 1160 / 2323, train loss = 5.439, lr = 0.420, \n",
            "batch no = 1392 / 2323, train loss = 5.553, lr = 0.420, \n",
            "batch no = 1624 / 2323, train loss = 5.220, lr = 0.420, \n",
            "batch no = 1856 / 2323, train loss = 4.975, lr = 0.420, \n",
            "batch no = 2088 / 2323, train loss = 5.517, lr = 0.420, \n",
            "batch no = 2320 / 2323, train loss = 5.372, lr = 0.420, \n",
            "Validation set perplexity : 191.152\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 189.938\n",
            "Training over\n",
            "Epoch 2/15\n",
            "batch no = 0 / 2323, train loss = 5.877, lr = 0.420, \n",
            "batch no = 232 / 2323, train loss = 4.986, lr = 0.420, \n",
            "batch no = 464 / 2323, train loss = 5.093, lr = 0.420, \n",
            "batch no = 696 / 2323, train loss = 5.035, lr = 0.420, \n",
            "batch no = 928 / 2323, train loss = 4.721, lr = 0.420, \n",
            "batch no = 1160 / 2323, train loss = 4.957, lr = 0.420, \n",
            "batch no = 1392 / 2323, train loss = 5.093, lr = 0.420, \n",
            "batch no = 1624 / 2323, train loss = 4.848, lr = 0.420, \n",
            "batch no = 1856 / 2323, train loss = 4.539, lr = 0.420, \n",
            "batch no = 2088 / 2323, train loss = 5.013, lr = 0.420, \n",
            "batch no = 2320 / 2323, train loss = 4.935, lr = 0.420, \n",
            "Validation set perplexity : 150.554\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 150.019\n",
            "Training over\n",
            "Epoch 3/15\n",
            "batch no = 0 / 2323, train loss = 5.509, lr = 0.420, \n",
            "batch no = 232 / 2323, train loss = 4.658, lr = 0.420, \n",
            "batch no = 464 / 2323, train loss = 4.701, lr = 0.420, \n",
            "batch no = 696 / 2323, train loss = 4.674, lr = 0.420, \n",
            "batch no = 928 / 2323, train loss = 4.392, lr = 0.420, \n",
            "batch no = 1160 / 2323, train loss = 4.669, lr = 0.420, \n",
            "batch no = 1392 / 2323, train loss = 4.817, lr = 0.420, \n",
            "batch no = 1624 / 2323, train loss = 4.636, lr = 0.420, \n",
            "batch no = 1856 / 2323, train loss = 4.296, lr = 0.420, \n",
            "batch no = 2088 / 2323, train loss = 4.705, lr = 0.420, \n",
            "batch no = 2320 / 2323, train loss = 4.632, lr = 0.420, \n",
            "Validation set perplexity : 143.585\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 142.452\n",
            "Training over\n",
            "Epoch 4/15\n",
            "batch no = 0 / 2323, train loss = 5.249, lr = 0.420, \n",
            "batch no = 232 / 2323, train loss = 4.388, lr = 0.420, \n",
            "batch no = 464 / 2323, train loss = 4.497, lr = 0.420, \n",
            "batch no = 696 / 2323, train loss = 4.435, lr = 0.420, \n",
            "batch no = 928 / 2323, train loss = 4.244, lr = 0.420, \n",
            "batch no = 1160 / 2323, train loss = 4.495, lr = 0.420, \n",
            "batch no = 1392 / 2323, train loss = 4.604, lr = 0.420, \n",
            "batch no = 1624 / 2323, train loss = 4.451, lr = 0.420, \n",
            "batch no = 1856 / 2323, train loss = 4.116, lr = 0.420, \n",
            "batch no = 2088 / 2323, train loss = 4.518, lr = 0.420, \n",
            "batch no = 2320 / 2323, train loss = 4.436, lr = 0.420, \n",
            "Validation set perplexity : 144.757\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 142.687\n",
            "Training over\n",
            "Epoch 5/15\n",
            "batch no = 0 / 2323, train loss = 5.043, lr = 0.350, \n",
            "batch no = 232 / 2323, train loss = 4.163, lr = 0.350, \n",
            "batch no = 464 / 2323, train loss = 4.299, lr = 0.350, \n",
            "batch no = 696 / 2323, train loss = 4.239, lr = 0.350, \n",
            "batch no = 928 / 2323, train loss = 4.037, lr = 0.350, \n",
            "batch no = 1160 / 2323, train loss = 4.310, lr = 0.350, \n",
            "batch no = 1392 / 2323, train loss = 4.339, lr = 0.350, \n",
            "batch no = 1624 / 2323, train loss = 4.227, lr = 0.350, \n",
            "batch no = 1856 / 2323, train loss = 3.923, lr = 0.350, \n",
            "batch no = 2088 / 2323, train loss = 4.287, lr = 0.350, \n",
            "batch no = 2320 / 2323, train loss = 4.232, lr = 0.350, \n",
            "Validation set perplexity : 142.723\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 139.931\n",
            "Training over\n",
            "Epoch 6/15\n",
            "batch no = 0 / 2323, train loss = 4.862, lr = 0.292, \n",
            "batch no = 232 / 2323, train loss = 3.964, lr = 0.292, \n",
            "batch no = 464 / 2323, train loss = 4.114, lr = 0.292, \n",
            "batch no = 696 / 2323, train loss = 4.112, lr = 0.292, \n",
            "batch no = 928 / 2323, train loss = 3.869, lr = 0.292, \n",
            "batch no = 1160 / 2323, train loss = 4.138, lr = 0.292, \n",
            "batch no = 1392 / 2323, train loss = 4.176, lr = 0.292, \n",
            "batch no = 1624 / 2323, train loss = 4.039, lr = 0.292, \n",
            "batch no = 1856 / 2323, train loss = 3.752, lr = 0.292, \n",
            "batch no = 2088 / 2323, train loss = 4.100, lr = 0.292, \n",
            "batch no = 2320 / 2323, train loss = 4.072, lr = 0.292, \n",
            "Validation set perplexity : 144.495\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 141.311\n",
            "Training over\n",
            "Epoch 7/15\n",
            "batch no = 0 / 2323, train loss = 4.757, lr = 0.243, \n",
            "batch no = 232 / 2323, train loss = 3.827, lr = 0.243, \n",
            "batch no = 464 / 2323, train loss = 3.972, lr = 0.243, \n",
            "batch no = 696 / 2323, train loss = 3.973, lr = 0.243, \n",
            "batch no = 928 / 2323, train loss = 3.712, lr = 0.243, \n",
            "batch no = 1160 / 2323, train loss = 3.992, lr = 0.243, \n",
            "batch no = 1392 / 2323, train loss = 4.053, lr = 0.243, \n",
            "batch no = 1624 / 2323, train loss = 3.936, lr = 0.243, \n",
            "batch no = 1856 / 2323, train loss = 3.585, lr = 0.243, \n",
            "batch no = 2088 / 2323, train loss = 3.905, lr = 0.243, \n",
            "batch no = 2320 / 2323, train loss = 3.955, lr = 0.243, \n",
            "Validation set perplexity : 145.795\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 143.028\n",
            "Training over\n",
            "Epoch 8/15\n",
            "batch no = 0 / 2323, train loss = 4.636, lr = 0.203, \n",
            "batch no = 232 / 2323, train loss = 3.723, lr = 0.203, \n",
            "batch no = 464 / 2323, train loss = 3.832, lr = 0.203, \n",
            "batch no = 696 / 2323, train loss = 3.879, lr = 0.203, \n",
            "batch no = 928 / 2323, train loss = 3.590, lr = 0.203, \n",
            "batch no = 1160 / 2323, train loss = 3.871, lr = 0.203, \n",
            "batch no = 1392 / 2323, train loss = 3.922, lr = 0.203, \n",
            "batch no = 1624 / 2323, train loss = 3.854, lr = 0.203, \n",
            "batch no = 1856 / 2323, train loss = 3.505, lr = 0.203, \n",
            "batch no = 2088 / 2323, train loss = 3.764, lr = 0.203, \n",
            "batch no = 2320 / 2323, train loss = 3.873, lr = 0.203, \n",
            "Validation set perplexity : 148.637\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 145.795\n",
            "Training over\n",
            "Epoch 9/15\n",
            "batch no = 0 / 2323, train loss = 4.534, lr = 0.169, \n",
            "batch no = 232 / 2323, train loss = 3.617, lr = 0.169, \n",
            "batch no = 464 / 2323, train loss = 3.750, lr = 0.169, \n",
            "batch no = 696 / 2323, train loss = 3.743, lr = 0.169, \n",
            "batch no = 928 / 2323, train loss = 3.485, lr = 0.169, \n",
            "batch no = 1160 / 2323, train loss = 3.761, lr = 0.169, \n",
            "batch no = 1392 / 2323, train loss = 3.771, lr = 0.169, \n",
            "batch no = 1624 / 2323, train loss = 3.752, lr = 0.169, \n",
            "batch no = 1856 / 2323, train loss = 3.418, lr = 0.169, \n",
            "batch no = 2088 / 2323, train loss = 3.658, lr = 0.169, \n",
            "batch no = 2320 / 2323, train loss = 3.778, lr = 0.169, \n",
            "Validation set perplexity : 152.391\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 149.318\n",
            "Training over\n",
            "Epoch 10/15\n",
            "batch no = 0 / 2323, train loss = 4.416, lr = 0.141, \n",
            "batch no = 232 / 2323, train loss = 3.557, lr = 0.141, \n",
            "batch no = 464 / 2323, train loss = 3.677, lr = 0.141, \n",
            "batch no = 696 / 2323, train loss = 3.626, lr = 0.141, \n",
            "batch no = 928 / 2323, train loss = 3.407, lr = 0.141, \n",
            "batch no = 1160 / 2323, train loss = 3.659, lr = 0.141, \n",
            "batch no = 1392 / 2323, train loss = 3.649, lr = 0.141, \n",
            "batch no = 1624 / 2323, train loss = 3.671, lr = 0.141, \n",
            "batch no = 1856 / 2323, train loss = 3.372, lr = 0.141, \n",
            "batch no = 2088 / 2323, train loss = 3.569, lr = 0.141, \n",
            "batch no = 2320 / 2323, train loss = 3.673, lr = 0.141, \n",
            "Validation set perplexity : 155.830\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 152.472\n",
            "Training over\n",
            "Epoch 11/15\n",
            "batch no = 0 / 2323, train loss = 4.336, lr = 0.117, \n",
            "batch no = 232 / 2323, train loss = 3.483, lr = 0.117, \n",
            "batch no = 464 / 2323, train loss = 3.616, lr = 0.117, \n",
            "batch no = 696 / 2323, train loss = 3.536, lr = 0.117, \n",
            "batch no = 928 / 2323, train loss = 3.348, lr = 0.117, \n",
            "batch no = 1160 / 2323, train loss = 3.559, lr = 0.117, \n",
            "batch no = 1392 / 2323, train loss = 3.549, lr = 0.117, \n",
            "batch no = 1624 / 2323, train loss = 3.581, lr = 0.117, \n",
            "batch no = 1856 / 2323, train loss = 3.315, lr = 0.117, \n",
            "batch no = 2088 / 2323, train loss = 3.493, lr = 0.117, \n",
            "batch no = 2320 / 2323, train loss = 3.588, lr = 0.117, \n",
            "Validation set perplexity : 159.742\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 156.153\n",
            "Training over\n",
            "Epoch 12/15\n",
            "batch no = 0 / 2323, train loss = 4.248, lr = 0.098, \n",
            "batch no = 232 / 2323, train loss = 3.408, lr = 0.098, \n",
            "batch no = 464 / 2323, train loss = 3.554, lr = 0.098, \n",
            "batch no = 696 / 2323, train loss = 3.482, lr = 0.098, \n",
            "batch no = 928 / 2323, train loss = 3.286, lr = 0.098, \n",
            "batch no = 1160 / 2323, train loss = 3.495, lr = 0.098, \n",
            "batch no = 1392 / 2323, train loss = 3.478, lr = 0.098, \n",
            "batch no = 1624 / 2323, train loss = 3.510, lr = 0.098, \n",
            "batch no = 1856 / 2323, train loss = 3.266, lr = 0.098, \n",
            "batch no = 2088 / 2323, train loss = 3.446, lr = 0.098, \n",
            "batch no = 2320 / 2323, train loss = 3.526, lr = 0.098, \n",
            "Validation set perplexity : 162.950\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 159.645\n",
            "Training over\n",
            "Epoch 13/15\n",
            "batch no = 0 / 2323, train loss = 4.186, lr = 0.081, \n",
            "batch no = 232 / 2323, train loss = 3.338, lr = 0.081, \n",
            "batch no = 464 / 2323, train loss = 3.528, lr = 0.081, \n",
            "batch no = 696 / 2323, train loss = 3.422, lr = 0.081, \n",
            "batch no = 928 / 2323, train loss = 3.239, lr = 0.081, \n",
            "batch no = 1160 / 2323, train loss = 3.453, lr = 0.081, \n",
            "batch no = 1392 / 2323, train loss = 3.428, lr = 0.081, \n",
            "batch no = 1624 / 2323, train loss = 3.464, lr = 0.081, \n",
            "batch no = 1856 / 2323, train loss = 3.219, lr = 0.081, \n",
            "batch no = 2088 / 2323, train loss = 3.382, lr = 0.081, \n",
            "batch no = 2320 / 2323, train loss = 3.472, lr = 0.081, \n",
            "Validation set perplexity : 165.675\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 162.667\n",
            "Training over\n",
            "Epoch 14/15\n",
            "batch no = 0 / 2323, train loss = 4.134, lr = 0.068, \n",
            "batch no = 232 / 2323, train loss = 3.297, lr = 0.068, \n",
            "batch no = 464 / 2323, train loss = 3.504, lr = 0.068, \n",
            "batch no = 696 / 2323, train loss = 3.382, lr = 0.068, \n",
            "batch no = 928 / 2323, train loss = 3.190, lr = 0.068, \n",
            "batch no = 1160 / 2323, train loss = 3.414, lr = 0.068, \n",
            "batch no = 1392 / 2323, train loss = 3.394, lr = 0.068, \n",
            "batch no = 1624 / 2323, train loss = 3.417, lr = 0.068, \n",
            "batch no = 1856 / 2323, train loss = 3.160, lr = 0.068, \n",
            "batch no = 2088 / 2323, train loss = 3.325, lr = 0.068, \n",
            "batch no = 2320 / 2323, train loss = 3.416, lr = 0.068, \n",
            "Validation set perplexity : 168.405\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 165.280\n",
            "Training over\n",
            "Epoch 15/15\n",
            "batch no = 0 / 2323, train loss = 4.083, lr = 0.057, \n",
            "batch no = 232 / 2323, train loss = 3.259, lr = 0.057, \n",
            "batch no = 464 / 2323, train loss = 3.486, lr = 0.057, \n",
            "batch no = 696 / 2323, train loss = 3.342, lr = 0.057, \n",
            "batch no = 928 / 2323, train loss = 3.139, lr = 0.057, \n",
            "batch no = 1160 / 2323, train loss = 3.376, lr = 0.057, \n",
            "batch no = 1392 / 2323, train loss = 3.368, lr = 0.057, \n",
            "batch no = 1624 / 2323, train loss = 3.384, lr = 0.057, \n",
            "batch no = 1856 / 2323, train loss = 3.115, lr = 0.057, \n",
            "batch no = 2088 / 2323, train loss = 3.272, lr = 0.057, \n",
            "batch no = 2320 / 2323, train loss = 3.372, lr = 0.057, \n",
            "Validation set perplexity : 170.973\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 167.573\n",
            "Training over\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru_dropout = Model(words_len,200,2,0.4,0.1,'gru') #GRU model 40% dropout\n",
        "model_gru_dropout.to(device)\n",
        "num_epochs = 50\n",
        "lr = 0.5\n",
        "lr_factor = 1.2\n",
        "epoch_threshold = 15\n",
        "main((train,validation,test), num_epochs, epoch_threshold, lr, model_gru_dropout,\"GRU_dropout\",'gru',lr_factor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVoYB8jVKtkL",
        "outputId": "2a674690-2232-478b-b951-da5fc630a737"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "batch no = 0 / 2323, train loss = 9.207, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 6.380, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 6.073, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 6.045, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 5.507, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 5.661, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 5.752, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 5.510, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 5.312, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 5.824, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 5.694, lr = 0.500, \n",
            "Validation set perplexity : 223.914\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 220.564\n",
            "Training over\n",
            "Epoch 2/50\n",
            "batch no = 0 / 2323, train loss = 6.034, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 5.425, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 5.379, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 5.442, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 5.135, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 5.343, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 5.435, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 5.265, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.956, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 5.467, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 5.451, lr = 0.500, \n",
            "Validation set perplexity : 175.047\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 173.393\n",
            "Training over\n",
            "Epoch 3/50\n",
            "batch no = 0 / 2323, train loss = 5.800, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 5.167, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 5.283, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 5.264, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.943, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 5.283, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 5.308, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 5.080, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.854, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 5.279, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 5.173, lr = 0.500, \n",
            "Validation set perplexity : 159.707\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 157.225\n",
            "Training over\n",
            "Epoch 4/50\n",
            "batch no = 0 / 2323, train loss = 5.679, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.994, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 5.115, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 5.117, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.878, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 5.222, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 5.247, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 5.052, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.729, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 5.224, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 5.073, lr = 0.500, \n",
            "Validation set perplexity : 150.150\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 148.438\n",
            "Training over\n",
            "Epoch 5/50\n",
            "batch no = 0 / 2323, train loss = 5.578, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.879, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 5.027, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 5.069, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.849, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 5.017, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 5.191, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.929, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.669, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 5.193, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 5.098, lr = 0.500, \n",
            "Validation set perplexity : 142.143\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 140.002\n",
            "Training over\n",
            "Epoch 6/50\n",
            "batch no = 0 / 2323, train loss = 5.609, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.872, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 4.984, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 4.932, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.776, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 5.023, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 5.080, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.889, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.592, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 5.048, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 5.011, lr = 0.500, \n",
            "Validation set perplexity : 137.596\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 135.206\n",
            "Training over\n",
            "Epoch 7/50\n",
            "batch no = 0 / 2323, train loss = 5.533, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.783, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 4.920, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 4.955, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.693, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 4.999, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 5.066, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.818, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.614, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 4.975, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 4.919, lr = 0.500, \n",
            "Validation set perplexity : 132.723\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 130.285\n",
            "Training over\n",
            "Epoch 8/50\n",
            "batch no = 0 / 2323, train loss = 5.456, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.727, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 4.950, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 4.856, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.698, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 4.927, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 5.093, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.827, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.596, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 4.977, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 4.928, lr = 0.500, \n",
            "Validation set perplexity : 130.455\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 127.956\n",
            "Training over\n",
            "Epoch 9/50\n",
            "batch no = 0 / 2323, train loss = 5.501, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.739, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 4.882, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 4.792, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.628, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 4.806, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 4.960, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.818, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.566, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 4.951, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 4.890, lr = 0.500, \n",
            "Validation set perplexity : 128.686\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 126.543\n",
            "Training over\n",
            "Epoch 10/50\n",
            "batch no = 0 / 2323, train loss = 5.506, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.671, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 4.803, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 4.790, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.574, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 4.841, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 4.957, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.688, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.472, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 4.982, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 4.758, lr = 0.500, \n",
            "Validation set perplexity : 128.368\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 126.358\n",
            "Training over\n",
            "Epoch 11/50\n",
            "batch no = 0 / 2323, train loss = 5.467, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.640, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 4.793, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 4.745, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.523, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 4.809, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 4.893, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.762, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.514, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 4.936, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 4.800, lr = 0.500, \n",
            "Validation set perplexity : 126.917\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 125.255\n",
            "Training over\n",
            "Epoch 12/50\n",
            "batch no = 0 / 2323, train loss = 5.404, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.674, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 4.845, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 4.741, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.606, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 4.820, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 4.881, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.682, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.410, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 4.828, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 4.772, lr = 0.500, \n",
            "Validation set perplexity : 126.736\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 123.972\n",
            "Training over\n",
            "Epoch 13/50\n",
            "batch no = 0 / 2323, train loss = 5.368, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.577, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 4.693, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 4.660, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.601, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 4.747, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 4.817, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.711, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.463, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 4.958, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 4.787, lr = 0.500, \n",
            "Validation set perplexity : 123.213\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 119.912\n",
            "Training over\n",
            "Epoch 14/50\n",
            "batch no = 0 / 2323, train loss = 5.423, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.551, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 4.802, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 4.678, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.469, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 4.863, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 4.911, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.772, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.403, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 4.912, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 4.654, lr = 0.500, \n",
            "Validation set perplexity : 122.803\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 120.194\n",
            "Training over\n",
            "Epoch 15/50\n",
            "batch no = 0 / 2323, train loss = 5.404, lr = 0.500, \n",
            "batch no = 232 / 2323, train loss = 4.563, lr = 0.500, \n",
            "batch no = 464 / 2323, train loss = 4.694, lr = 0.500, \n",
            "batch no = 696 / 2323, train loss = 4.602, lr = 0.500, \n",
            "batch no = 928 / 2323, train loss = 4.505, lr = 0.500, \n",
            "batch no = 1160 / 2323, train loss = 4.742, lr = 0.500, \n",
            "batch no = 1392 / 2323, train loss = 4.864, lr = 0.500, \n",
            "batch no = 1624 / 2323, train loss = 4.694, lr = 0.500, \n",
            "batch no = 1856 / 2323, train loss = 4.363, lr = 0.500, \n",
            "batch no = 2088 / 2323, train loss = 4.839, lr = 0.500, \n",
            "batch no = 2320 / 2323, train loss = 4.711, lr = 0.500, \n",
            "Validation set perplexity : 122.452\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 119.585\n",
            "Training over\n",
            "Epoch 16/50\n",
            "batch no = 0 / 2323, train loss = 5.389, lr = 0.417, \n",
            "batch no = 232 / 2323, train loss = 4.475, lr = 0.417, \n",
            "batch no = 464 / 2323, train loss = 4.669, lr = 0.417, \n",
            "batch no = 696 / 2323, train loss = 4.699, lr = 0.417, \n",
            "batch no = 928 / 2323, train loss = 4.529, lr = 0.417, \n",
            "batch no = 1160 / 2323, train loss = 4.737, lr = 0.417, \n",
            "batch no = 1392 / 2323, train loss = 4.756, lr = 0.417, \n",
            "batch no = 1624 / 2323, train loss = 4.679, lr = 0.417, \n",
            "batch no = 1856 / 2323, train loss = 4.355, lr = 0.417, \n",
            "batch no = 2088 / 2323, train loss = 4.710, lr = 0.417, \n",
            "batch no = 2320 / 2323, train loss = 4.586, lr = 0.417, \n",
            "Validation set perplexity : 119.390\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 115.968\n",
            "Training over\n",
            "Epoch 17/50\n",
            "batch no = 0 / 2323, train loss = 5.353, lr = 0.347, \n",
            "batch no = 232 / 2323, train loss = 4.428, lr = 0.347, \n",
            "batch no = 464 / 2323, train loss = 4.634, lr = 0.347, \n",
            "batch no = 696 / 2323, train loss = 4.564, lr = 0.347, \n",
            "batch no = 928 / 2323, train loss = 4.363, lr = 0.347, \n",
            "batch no = 1160 / 2323, train loss = 4.668, lr = 0.347, \n",
            "batch no = 1392 / 2323, train loss = 4.629, lr = 0.347, \n",
            "batch no = 1624 / 2323, train loss = 4.548, lr = 0.347, \n",
            "batch no = 1856 / 2323, train loss = 4.322, lr = 0.347, \n",
            "batch no = 2088 / 2323, train loss = 4.711, lr = 0.347, \n",
            "batch no = 2320 / 2323, train loss = 4.616, lr = 0.347, \n",
            "Validation set perplexity : 115.845\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 112.427\n",
            "Training over\n",
            "Epoch 18/50\n",
            "batch no = 0 / 2323, train loss = 5.294, lr = 0.289, \n",
            "batch no = 232 / 2323, train loss = 4.410, lr = 0.289, \n",
            "batch no = 464 / 2323, train loss = 4.534, lr = 0.289, \n",
            "batch no = 696 / 2323, train loss = 4.551, lr = 0.289, \n",
            "batch no = 928 / 2323, train loss = 4.419, lr = 0.289, \n",
            "batch no = 1160 / 2323, train loss = 4.695, lr = 0.289, \n",
            "batch no = 1392 / 2323, train loss = 4.673, lr = 0.289, \n",
            "batch no = 1624 / 2323, train loss = 4.526, lr = 0.289, \n",
            "batch no = 1856 / 2323, train loss = 4.251, lr = 0.289, \n",
            "batch no = 2088 / 2323, train loss = 4.693, lr = 0.289, \n",
            "batch no = 2320 / 2323, train loss = 4.622, lr = 0.289, \n",
            "Validation set perplexity : 113.217\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 109.871\n",
            "Training over\n",
            "Epoch 19/50\n",
            "batch no = 0 / 2323, train loss = 5.258, lr = 0.241, \n",
            "batch no = 232 / 2323, train loss = 4.452, lr = 0.241, \n",
            "batch no = 464 / 2323, train loss = 4.488, lr = 0.241, \n",
            "batch no = 696 / 2323, train loss = 4.526, lr = 0.241, \n",
            "batch no = 928 / 2323, train loss = 4.395, lr = 0.241, \n",
            "batch no = 1160 / 2323, train loss = 4.589, lr = 0.241, \n",
            "batch no = 1392 / 2323, train loss = 4.633, lr = 0.241, \n",
            "batch no = 1624 / 2323, train loss = 4.478, lr = 0.241, \n",
            "batch no = 1856 / 2323, train loss = 4.220, lr = 0.241, \n",
            "batch no = 2088 / 2323, train loss = 4.667, lr = 0.241, \n",
            "batch no = 2320 / 2323, train loss = 4.538, lr = 0.241, \n",
            "Validation set perplexity : 110.298\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 107.368\n",
            "Training over\n",
            "Epoch 20/50\n",
            "batch no = 0 / 2323, train loss = 5.167, lr = 0.201, \n",
            "batch no = 232 / 2323, train loss = 4.387, lr = 0.201, \n",
            "batch no = 464 / 2323, train loss = 4.438, lr = 0.201, \n",
            "batch no = 696 / 2323, train loss = 4.398, lr = 0.201, \n",
            "batch no = 928 / 2323, train loss = 4.359, lr = 0.201, \n",
            "batch no = 1160 / 2323, train loss = 4.667, lr = 0.201, \n",
            "batch no = 1392 / 2323, train loss = 4.640, lr = 0.201, \n",
            "batch no = 1624 / 2323, train loss = 4.426, lr = 0.201, \n",
            "batch no = 1856 / 2323, train loss = 4.186, lr = 0.201, \n",
            "batch no = 2088 / 2323, train loss = 4.559, lr = 0.201, \n",
            "batch no = 2320 / 2323, train loss = 4.518, lr = 0.201, \n",
            "Validation set perplexity : 108.352\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 105.218\n",
            "Training over\n",
            "Epoch 21/50\n",
            "batch no = 0 / 2323, train loss = 5.212, lr = 0.167, \n",
            "batch no = 232 / 2323, train loss = 4.283, lr = 0.167, \n",
            "batch no = 464 / 2323, train loss = 4.448, lr = 0.167, \n",
            "batch no = 696 / 2323, train loss = 4.446, lr = 0.167, \n",
            "batch no = 928 / 2323, train loss = 4.330, lr = 0.167, \n",
            "batch no = 1160 / 2323, train loss = 4.560, lr = 0.167, \n",
            "batch no = 1392 / 2323, train loss = 4.655, lr = 0.167, \n",
            "batch no = 1624 / 2323, train loss = 4.404, lr = 0.167, \n",
            "batch no = 1856 / 2323, train loss = 4.152, lr = 0.167, \n",
            "batch no = 2088 / 2323, train loss = 4.637, lr = 0.167, \n",
            "batch no = 2320 / 2323, train loss = 4.421, lr = 0.167, \n",
            "Validation set perplexity : 106.993\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 103.710\n",
            "Training over\n",
            "Epoch 22/50\n",
            "batch no = 0 / 2323, train loss = 5.217, lr = 0.140, \n",
            "batch no = 232 / 2323, train loss = 4.299, lr = 0.140, \n",
            "batch no = 464 / 2323, train loss = 4.487, lr = 0.140, \n",
            "batch no = 696 / 2323, train loss = 4.377, lr = 0.140, \n",
            "batch no = 928 / 2323, train loss = 4.206, lr = 0.140, \n",
            "batch no = 1160 / 2323, train loss = 4.506, lr = 0.140, \n",
            "batch no = 1392 / 2323, train loss = 4.550, lr = 0.140, \n",
            "batch no = 1624 / 2323, train loss = 4.451, lr = 0.140, \n",
            "batch no = 1856 / 2323, train loss = 4.189, lr = 0.140, \n",
            "batch no = 2088 / 2323, train loss = 4.525, lr = 0.140, \n",
            "batch no = 2320 / 2323, train loss = 4.465, lr = 0.140, \n",
            "Validation set perplexity : 105.782\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 102.455\n",
            "Training over\n",
            "Epoch 23/50\n",
            "batch no = 0 / 2323, train loss = 5.210, lr = 0.116, \n",
            "batch no = 232 / 2323, train loss = 4.245, lr = 0.116, \n",
            "batch no = 464 / 2323, train loss = 4.395, lr = 0.116, \n",
            "batch no = 696 / 2323, train loss = 4.293, lr = 0.116, \n",
            "batch no = 928 / 2323, train loss = 4.235, lr = 0.116, \n",
            "batch no = 1160 / 2323, train loss = 4.549, lr = 0.116, \n",
            "batch no = 1392 / 2323, train loss = 4.564, lr = 0.116, \n",
            "batch no = 1624 / 2323, train loss = 4.272, lr = 0.116, \n",
            "batch no = 1856 / 2323, train loss = 4.171, lr = 0.116, \n",
            "batch no = 2088 / 2323, train loss = 4.551, lr = 0.116, \n",
            "batch no = 2320 / 2323, train loss = 4.439, lr = 0.116, \n",
            "Validation set perplexity : 104.745\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 101.535\n",
            "Training over\n",
            "Epoch 24/50\n",
            "batch no = 0 / 2323, train loss = 5.188, lr = 0.097, \n",
            "batch no = 232 / 2323, train loss = 4.303, lr = 0.097, \n",
            "batch no = 464 / 2323, train loss = 4.462, lr = 0.097, \n",
            "batch no = 696 / 2323, train loss = 4.307, lr = 0.097, \n",
            "batch no = 928 / 2323, train loss = 4.176, lr = 0.097, \n",
            "batch no = 1160 / 2323, train loss = 4.578, lr = 0.097, \n",
            "batch no = 1392 / 2323, train loss = 4.488, lr = 0.097, \n",
            "batch no = 1624 / 2323, train loss = 4.388, lr = 0.097, \n",
            "batch no = 1856 / 2323, train loss = 4.131, lr = 0.097, \n",
            "batch no = 2088 / 2323, train loss = 4.488, lr = 0.097, \n",
            "batch no = 2320 / 2323, train loss = 4.362, lr = 0.097, \n",
            "Validation set perplexity : 103.782\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 100.436\n",
            "Training over\n",
            "Epoch 25/50\n",
            "batch no = 0 / 2323, train loss = 5.105, lr = 0.081, \n",
            "batch no = 232 / 2323, train loss = 4.176, lr = 0.081, \n",
            "batch no = 464 / 2323, train loss = 4.299, lr = 0.081, \n",
            "batch no = 696 / 2323, train loss = 4.316, lr = 0.081, \n",
            "batch no = 928 / 2323, train loss = 4.174, lr = 0.081, \n",
            "batch no = 1160 / 2323, train loss = 4.567, lr = 0.081, \n",
            "batch no = 1392 / 2323, train loss = 4.463, lr = 0.081, \n",
            "batch no = 1624 / 2323, train loss = 4.338, lr = 0.081, \n",
            "batch no = 1856 / 2323, train loss = 4.136, lr = 0.081, \n",
            "batch no = 2088 / 2323, train loss = 4.499, lr = 0.081, \n",
            "batch no = 2320 / 2323, train loss = 4.372, lr = 0.081, \n",
            "Validation set perplexity : 103.083\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 99.632\n",
            "Training over\n",
            "Epoch 26/50\n",
            "batch no = 0 / 2323, train loss = 5.111, lr = 0.067, \n",
            "batch no = 232 / 2323, train loss = 4.346, lr = 0.067, \n",
            "batch no = 464 / 2323, train loss = 4.375, lr = 0.067, \n",
            "batch no = 696 / 2323, train loss = 4.373, lr = 0.067, \n",
            "batch no = 928 / 2323, train loss = 4.206, lr = 0.067, \n",
            "batch no = 1160 / 2323, train loss = 4.424, lr = 0.067, \n",
            "batch no = 1392 / 2323, train loss = 4.483, lr = 0.067, \n",
            "batch no = 1624 / 2323, train loss = 4.378, lr = 0.067, \n",
            "batch no = 1856 / 2323, train loss = 4.084, lr = 0.067, \n",
            "batch no = 2088 / 2323, train loss = 4.500, lr = 0.067, \n",
            "batch no = 2320 / 2323, train loss = 4.441, lr = 0.067, \n",
            "Validation set perplexity : 102.289\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 98.852\n",
            "Training over\n",
            "Epoch 27/50\n",
            "batch no = 0 / 2323, train loss = 5.186, lr = 0.056, \n",
            "batch no = 232 / 2323, train loss = 4.281, lr = 0.056, \n",
            "batch no = 464 / 2323, train loss = 4.434, lr = 0.056, \n",
            "batch no = 696 / 2323, train loss = 4.406, lr = 0.056, \n",
            "batch no = 928 / 2323, train loss = 4.093, lr = 0.056, \n",
            "batch no = 1160 / 2323, train loss = 4.528, lr = 0.056, \n",
            "batch no = 1392 / 2323, train loss = 4.447, lr = 0.056, \n",
            "batch no = 1624 / 2323, train loss = 4.356, lr = 0.056, \n",
            "batch no = 1856 / 2323, train loss = 4.074, lr = 0.056, \n",
            "batch no = 2088 / 2323, train loss = 4.535, lr = 0.056, \n",
            "batch no = 2320 / 2323, train loss = 4.407, lr = 0.056, \n",
            "Validation set perplexity : 101.662\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 98.093\n",
            "Training over\n",
            "Epoch 28/50\n",
            "batch no = 0 / 2323, train loss = 5.059, lr = 0.047, \n",
            "batch no = 232 / 2323, train loss = 4.234, lr = 0.047, \n",
            "batch no = 464 / 2323, train loss = 4.381, lr = 0.047, \n",
            "batch no = 696 / 2323, train loss = 4.317, lr = 0.047, \n",
            "batch no = 928 / 2323, train loss = 4.162, lr = 0.047, \n",
            "batch no = 1160 / 2323, train loss = 4.570, lr = 0.047, \n",
            "batch no = 1392 / 2323, train loss = 4.431, lr = 0.047, \n",
            "batch no = 1624 / 2323, train loss = 4.316, lr = 0.047, \n",
            "batch no = 1856 / 2323, train loss = 4.106, lr = 0.047, \n",
            "batch no = 2088 / 2323, train loss = 4.541, lr = 0.047, \n",
            "batch no = 2320 / 2323, train loss = 4.399, lr = 0.047, \n",
            "Validation set perplexity : 101.360\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 97.794\n",
            "Training over\n",
            "Epoch 29/50\n",
            "batch no = 0 / 2323, train loss = 5.193, lr = 0.039, \n",
            "batch no = 232 / 2323, train loss = 4.204, lr = 0.039, \n",
            "batch no = 464 / 2323, train loss = 4.384, lr = 0.039, \n",
            "batch no = 696 / 2323, train loss = 4.290, lr = 0.039, \n",
            "batch no = 928 / 2323, train loss = 4.123, lr = 0.039, \n",
            "batch no = 1160 / 2323, train loss = 4.480, lr = 0.039, \n",
            "batch no = 1392 / 2323, train loss = 4.477, lr = 0.039, \n",
            "batch no = 1624 / 2323, train loss = 4.309, lr = 0.039, \n",
            "batch no = 1856 / 2323, train loss = 4.036, lr = 0.039, \n",
            "batch no = 2088 / 2323, train loss = 4.506, lr = 0.039, \n",
            "batch no = 2320 / 2323, train loss = 4.462, lr = 0.039, \n",
            "Validation set perplexity : 100.946\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 97.390\n",
            "Training over\n",
            "Epoch 30/50\n",
            "batch no = 0 / 2323, train loss = 5.146, lr = 0.032, \n",
            "batch no = 232 / 2323, train loss = 4.174, lr = 0.032, \n",
            "batch no = 464 / 2323, train loss = 4.466, lr = 0.032, \n",
            "batch no = 696 / 2323, train loss = 4.302, lr = 0.032, \n",
            "batch no = 928 / 2323, train loss = 4.103, lr = 0.032, \n",
            "batch no = 1160 / 2323, train loss = 4.470, lr = 0.032, \n",
            "batch no = 1392 / 2323, train loss = 4.491, lr = 0.032, \n",
            "batch no = 1624 / 2323, train loss = 4.355, lr = 0.032, \n",
            "batch no = 1856 / 2323, train loss = 4.048, lr = 0.032, \n",
            "batch no = 2088 / 2323, train loss = 4.538, lr = 0.032, \n",
            "batch no = 2320 / 2323, train loss = 4.406, lr = 0.032, \n",
            "Validation set perplexity : 100.744\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 97.122\n",
            "Training over\n",
            "Epoch 31/50\n",
            "batch no = 0 / 2323, train loss = 5.188, lr = 0.027, \n",
            "batch no = 232 / 2323, train loss = 4.119, lr = 0.027, \n",
            "batch no = 464 / 2323, train loss = 4.367, lr = 0.027, \n",
            "batch no = 696 / 2323, train loss = 4.330, lr = 0.027, \n",
            "batch no = 928 / 2323, train loss = 4.080, lr = 0.027, \n",
            "batch no = 1160 / 2323, train loss = 4.437, lr = 0.027, \n",
            "batch no = 1392 / 2323, train loss = 4.405, lr = 0.027, \n",
            "batch no = 1624 / 2323, train loss = 4.268, lr = 0.027, \n",
            "batch no = 1856 / 2323, train loss = 4.030, lr = 0.027, \n",
            "batch no = 2088 / 2323, train loss = 4.447, lr = 0.027, \n",
            "batch no = 2320 / 2323, train loss = 4.289, lr = 0.027, \n",
            "Validation set perplexity : 100.503\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.851\n",
            "Training over\n",
            "Epoch 32/50\n",
            "batch no = 0 / 2323, train loss = 5.148, lr = 0.023, \n",
            "batch no = 232 / 2323, train loss = 4.182, lr = 0.023, \n",
            "batch no = 464 / 2323, train loss = 4.237, lr = 0.023, \n",
            "batch no = 696 / 2323, train loss = 4.254, lr = 0.023, \n",
            "batch no = 928 / 2323, train loss = 4.145, lr = 0.023, \n",
            "batch no = 1160 / 2323, train loss = 4.477, lr = 0.023, \n",
            "batch no = 1392 / 2323, train loss = 4.428, lr = 0.023, \n",
            "batch no = 1624 / 2323, train loss = 4.298, lr = 0.023, \n",
            "batch no = 1856 / 2323, train loss = 4.019, lr = 0.023, \n",
            "batch no = 2088 / 2323, train loss = 4.429, lr = 0.023, \n",
            "batch no = 2320 / 2323, train loss = 4.411, lr = 0.023, \n",
            "Validation set perplexity : 100.305\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.652\n",
            "Training over\n",
            "Epoch 33/50\n",
            "batch no = 0 / 2323, train loss = 5.122, lr = 0.019, \n",
            "batch no = 232 / 2323, train loss = 4.337, lr = 0.019, \n",
            "batch no = 464 / 2323, train loss = 4.356, lr = 0.019, \n",
            "batch no = 696 / 2323, train loss = 4.295, lr = 0.019, \n",
            "batch no = 928 / 2323, train loss = 4.067, lr = 0.019, \n",
            "batch no = 1160 / 2323, train loss = 4.451, lr = 0.019, \n",
            "batch no = 1392 / 2323, train loss = 4.411, lr = 0.019, \n",
            "batch no = 1624 / 2323, train loss = 4.328, lr = 0.019, \n",
            "batch no = 1856 / 2323, train loss = 4.039, lr = 0.019, \n",
            "batch no = 2088 / 2323, train loss = 4.520, lr = 0.019, \n",
            "batch no = 2320 / 2323, train loss = 4.389, lr = 0.019, \n",
            "Validation set perplexity : 100.234\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.664\n",
            "Training over\n",
            "Epoch 34/50\n",
            "batch no = 0 / 2323, train loss = 5.076, lr = 0.016, \n",
            "batch no = 232 / 2323, train loss = 4.253, lr = 0.016, \n",
            "batch no = 464 / 2323, train loss = 4.352, lr = 0.016, \n",
            "batch no = 696 / 2323, train loss = 4.229, lr = 0.016, \n",
            "batch no = 928 / 2323, train loss = 4.112, lr = 0.016, \n",
            "batch no = 1160 / 2323, train loss = 4.454, lr = 0.016, \n",
            "batch no = 1392 / 2323, train loss = 4.515, lr = 0.016, \n",
            "batch no = 1624 / 2323, train loss = 4.294, lr = 0.016, \n",
            "batch no = 1856 / 2323, train loss = 4.012, lr = 0.016, \n",
            "batch no = 2088 / 2323, train loss = 4.459, lr = 0.016, \n",
            "batch no = 2320 / 2323, train loss = 4.379, lr = 0.016, \n",
            "Validation set perplexity : 100.065\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.453\n",
            "Training over\n",
            "Epoch 35/50\n",
            "batch no = 0 / 2323, train loss = 5.162, lr = 0.013, \n",
            "batch no = 232 / 2323, train loss = 4.238, lr = 0.013, \n",
            "batch no = 464 / 2323, train loss = 4.456, lr = 0.013, \n",
            "batch no = 696 / 2323, train loss = 4.211, lr = 0.013, \n",
            "batch no = 928 / 2323, train loss = 4.071, lr = 0.013, \n",
            "batch no = 1160 / 2323, train loss = 4.433, lr = 0.013, \n",
            "batch no = 1392 / 2323, train loss = 4.375, lr = 0.013, \n",
            "batch no = 1624 / 2323, train loss = 4.302, lr = 0.013, \n",
            "batch no = 1856 / 2323, train loss = 4.069, lr = 0.013, \n",
            "batch no = 2088 / 2323, train loss = 4.473, lr = 0.013, \n",
            "batch no = 2320 / 2323, train loss = 4.442, lr = 0.013, \n",
            "Validation set perplexity : 100.137\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.449\n",
            "Training over\n",
            "Epoch 36/50\n",
            "batch no = 0 / 2323, train loss = 5.112, lr = 0.011, \n",
            "batch no = 232 / 2323, train loss = 4.294, lr = 0.011, \n",
            "batch no = 464 / 2323, train loss = 4.367, lr = 0.011, \n",
            "batch no = 696 / 2323, train loss = 4.236, lr = 0.011, \n",
            "batch no = 928 / 2323, train loss = 4.098, lr = 0.011, \n",
            "batch no = 1160 / 2323, train loss = 4.386, lr = 0.011, \n",
            "batch no = 1392 / 2323, train loss = 4.443, lr = 0.011, \n",
            "batch no = 1624 / 2323, train loss = 4.201, lr = 0.011, \n",
            "batch no = 1856 / 2323, train loss = 3.951, lr = 0.011, \n",
            "batch no = 2088 / 2323, train loss = 4.477, lr = 0.011, \n",
            "batch no = 2320 / 2323, train loss = 4.374, lr = 0.011, \n",
            "Validation set perplexity : 99.966\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.315\n",
            "Training over\n",
            "Epoch 37/50\n",
            "batch no = 0 / 2323, train loss = 5.127, lr = 0.009, \n",
            "batch no = 232 / 2323, train loss = 4.192, lr = 0.009, \n",
            "batch no = 464 / 2323, train loss = 4.376, lr = 0.009, \n",
            "batch no = 696 / 2323, train loss = 4.336, lr = 0.009, \n",
            "batch no = 928 / 2323, train loss = 4.137, lr = 0.009, \n",
            "batch no = 1160 / 2323, train loss = 4.393, lr = 0.009, \n",
            "batch no = 1392 / 2323, train loss = 4.451, lr = 0.009, \n",
            "batch no = 1624 / 2323, train loss = 4.305, lr = 0.009, \n",
            "batch no = 1856 / 2323, train loss = 4.027, lr = 0.009, \n",
            "batch no = 2088 / 2323, train loss = 4.493, lr = 0.009, \n",
            "batch no = 2320 / 2323, train loss = 4.377, lr = 0.009, \n",
            "Validation set perplexity : 99.888\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.250\n",
            "Training over\n",
            "Epoch 38/50\n",
            "batch no = 0 / 2323, train loss = 5.078, lr = 0.008, \n",
            "batch no = 232 / 2323, train loss = 4.208, lr = 0.008, \n",
            "batch no = 464 / 2323, train loss = 4.294, lr = 0.008, \n",
            "batch no = 696 / 2323, train loss = 4.365, lr = 0.008, \n",
            "batch no = 928 / 2323, train loss = 4.152, lr = 0.008, \n",
            "batch no = 1160 / 2323, train loss = 4.457, lr = 0.008, \n",
            "batch no = 1392 / 2323, train loss = 4.365, lr = 0.008, \n",
            "batch no = 1624 / 2323, train loss = 4.354, lr = 0.008, \n",
            "batch no = 1856 / 2323, train loss = 4.124, lr = 0.008, \n",
            "batch no = 2088 / 2323, train loss = 4.436, lr = 0.008, \n",
            "batch no = 2320 / 2323, train loss = 4.378, lr = 0.008, \n",
            "Validation set perplexity : 99.812\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.141\n",
            "Training over\n",
            "Epoch 39/50\n",
            "batch no = 0 / 2323, train loss = 5.091, lr = 0.006, \n",
            "batch no = 232 / 2323, train loss = 4.193, lr = 0.006, \n",
            "batch no = 464 / 2323, train loss = 4.350, lr = 0.006, \n",
            "batch no = 696 / 2323, train loss = 4.295, lr = 0.006, \n",
            "batch no = 928 / 2323, train loss = 4.142, lr = 0.006, \n",
            "batch no = 1160 / 2323, train loss = 4.494, lr = 0.006, \n",
            "batch no = 1392 / 2323, train loss = 4.412, lr = 0.006, \n",
            "batch no = 1624 / 2323, train loss = 4.296, lr = 0.006, \n",
            "batch no = 1856 / 2323, train loss = 4.010, lr = 0.006, \n",
            "batch no = 2088 / 2323, train loss = 4.424, lr = 0.006, \n",
            "batch no = 2320 / 2323, train loss = 4.344, lr = 0.006, \n",
            "Validation set perplexity : 99.908\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.247\n",
            "Training over\n",
            "Epoch 40/50\n",
            "batch no = 0 / 2323, train loss = 5.065, lr = 0.005, \n",
            "batch no = 232 / 2323, train loss = 4.216, lr = 0.005, \n",
            "batch no = 464 / 2323, train loss = 4.389, lr = 0.005, \n",
            "batch no = 696 / 2323, train loss = 4.306, lr = 0.005, \n",
            "batch no = 928 / 2323, train loss = 4.143, lr = 0.005, \n",
            "batch no = 1160 / 2323, train loss = 4.403, lr = 0.005, \n",
            "batch no = 1392 / 2323, train loss = 4.498, lr = 0.005, \n",
            "batch no = 1624 / 2323, train loss = 4.251, lr = 0.005, \n",
            "batch no = 1856 / 2323, train loss = 4.134, lr = 0.005, \n",
            "batch no = 2088 / 2323, train loss = 4.444, lr = 0.005, \n",
            "batch no = 2320 / 2323, train loss = 4.367, lr = 0.005, \n",
            "Validation set perplexity : 99.776\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.135\n",
            "Training over\n",
            "Epoch 41/50\n",
            "batch no = 0 / 2323, train loss = 5.114, lr = 0.004, \n",
            "batch no = 232 / 2323, train loss = 4.238, lr = 0.004, \n",
            "batch no = 464 / 2323, train loss = 4.375, lr = 0.004, \n",
            "batch no = 696 / 2323, train loss = 4.341, lr = 0.004, \n",
            "batch no = 928 / 2323, train loss = 4.101, lr = 0.004, \n",
            "batch no = 1160 / 2323, train loss = 4.416, lr = 0.004, \n",
            "batch no = 1392 / 2323, train loss = 4.410, lr = 0.004, \n",
            "batch no = 1624 / 2323, train loss = 4.341, lr = 0.004, \n",
            "batch no = 1856 / 2323, train loss = 4.061, lr = 0.004, \n",
            "batch no = 2088 / 2323, train loss = 4.486, lr = 0.004, \n",
            "batch no = 2320 / 2323, train loss = 4.427, lr = 0.004, \n",
            "Validation set perplexity : 99.741\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.096\n",
            "Training over\n",
            "Epoch 42/50\n",
            "batch no = 0 / 2323, train loss = 5.149, lr = 0.004, \n",
            "batch no = 232 / 2323, train loss = 4.205, lr = 0.004, \n",
            "batch no = 464 / 2323, train loss = 4.350, lr = 0.004, \n",
            "batch no = 696 / 2323, train loss = 4.238, lr = 0.004, \n",
            "batch no = 928 / 2323, train loss = 4.212, lr = 0.004, \n",
            "batch no = 1160 / 2323, train loss = 4.456, lr = 0.004, \n",
            "batch no = 1392 / 2323, train loss = 4.381, lr = 0.004, \n",
            "batch no = 1624 / 2323, train loss = 4.312, lr = 0.004, \n",
            "batch no = 1856 / 2323, train loss = 4.087, lr = 0.004, \n",
            "batch no = 2088 / 2323, train loss = 4.382, lr = 0.004, \n",
            "batch no = 2320 / 2323, train loss = 4.298, lr = 0.004, \n",
            "Validation set perplexity : 99.751\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 96.082\n",
            "Training over\n",
            "Epoch 43/50\n",
            "batch no = 0 / 2323, train loss = 5.052, lr = 0.003, \n",
            "batch no = 232 / 2323, train loss = 4.225, lr = 0.003, \n",
            "batch no = 464 / 2323, train loss = 4.375, lr = 0.003, \n",
            "batch no = 696 / 2323, train loss = 4.205, lr = 0.003, \n",
            "batch no = 928 / 2323, train loss = 4.081, lr = 0.003, \n",
            "batch no = 1160 / 2323, train loss = 4.478, lr = 0.003, \n",
            "batch no = 1392 / 2323, train loss = 4.398, lr = 0.003, \n",
            "batch no = 1624 / 2323, train loss = 4.291, lr = 0.003, \n",
            "batch no = 1856 / 2323, train loss = 4.092, lr = 0.003, \n",
            "batch no = 2088 / 2323, train loss = 4.389, lr = 0.003, \n",
            "batch no = 2320 / 2323, train loss = 4.359, lr = 0.003, \n",
            "Validation set perplexity : 99.649\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.984\n",
            "Training over\n",
            "Epoch 44/50\n",
            "batch no = 0 / 2323, train loss = 5.061, lr = 0.003, \n",
            "batch no = 232 / 2323, train loss = 4.200, lr = 0.003, \n",
            "batch no = 464 / 2323, train loss = 4.323, lr = 0.003, \n",
            "batch no = 696 / 2323, train loss = 4.251, lr = 0.003, \n",
            "batch no = 928 / 2323, train loss = 4.140, lr = 0.003, \n",
            "batch no = 1160 / 2323, train loss = 4.321, lr = 0.003, \n",
            "batch no = 1392 / 2323, train loss = 4.398, lr = 0.003, \n",
            "batch no = 1624 / 2323, train loss = 4.292, lr = 0.003, \n",
            "batch no = 1856 / 2323, train loss = 4.093, lr = 0.003, \n",
            "batch no = 2088 / 2323, train loss = 4.459, lr = 0.003, \n",
            "batch no = 2320 / 2323, train loss = 4.334, lr = 0.003, \n",
            "Validation set perplexity : 99.655\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.992\n",
            "Training over\n",
            "Epoch 45/50\n",
            "batch no = 0 / 2323, train loss = 5.130, lr = 0.002, \n",
            "batch no = 232 / 2323, train loss = 4.143, lr = 0.002, \n",
            "batch no = 464 / 2323, train loss = 4.338, lr = 0.002, \n",
            "batch no = 696 / 2323, train loss = 4.239, lr = 0.002, \n",
            "batch no = 928 / 2323, train loss = 4.146, lr = 0.002, \n",
            "batch no = 1160 / 2323, train loss = 4.468, lr = 0.002, \n",
            "batch no = 1392 / 2323, train loss = 4.426, lr = 0.002, \n",
            "batch no = 1624 / 2323, train loss = 4.247, lr = 0.002, \n",
            "batch no = 1856 / 2323, train loss = 4.039, lr = 0.002, \n",
            "batch no = 2088 / 2323, train loss = 4.453, lr = 0.002, \n",
            "batch no = 2320 / 2323, train loss = 4.266, lr = 0.002, \n",
            "Validation set perplexity : 99.627\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.945\n",
            "Training over\n",
            "Epoch 46/50\n",
            "batch no = 0 / 2323, train loss = 5.111, lr = 0.002, \n",
            "batch no = 232 / 2323, train loss = 4.217, lr = 0.002, \n",
            "batch no = 464 / 2323, train loss = 4.360, lr = 0.002, \n",
            "batch no = 696 / 2323, train loss = 4.292, lr = 0.002, \n",
            "batch no = 928 / 2323, train loss = 4.142, lr = 0.002, \n",
            "batch no = 1160 / 2323, train loss = 4.384, lr = 0.002, \n",
            "batch no = 1392 / 2323, train loss = 4.394, lr = 0.002, \n",
            "batch no = 1624 / 2323, train loss = 4.371, lr = 0.002, \n",
            "batch no = 1856 / 2323, train loss = 4.035, lr = 0.002, \n",
            "batch no = 2088 / 2323, train loss = 4.331, lr = 0.002, \n",
            "batch no = 2320 / 2323, train loss = 4.376, lr = 0.002, \n",
            "Validation set perplexity : 99.616\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.933\n",
            "Training over\n",
            "Epoch 47/50\n",
            "batch no = 0 / 2323, train loss = 5.113, lr = 0.001, \n",
            "batch no = 232 / 2323, train loss = 4.137, lr = 0.001, \n",
            "batch no = 464 / 2323, train loss = 4.317, lr = 0.001, \n",
            "batch no = 696 / 2323, train loss = 4.244, lr = 0.001, \n",
            "batch no = 928 / 2323, train loss = 4.131, lr = 0.001, \n",
            "batch no = 1160 / 2323, train loss = 4.475, lr = 0.001, \n",
            "batch no = 1392 / 2323, train loss = 4.377, lr = 0.001, \n",
            "batch no = 1624 / 2323, train loss = 4.284, lr = 0.001, \n",
            "batch no = 1856 / 2323, train loss = 4.105, lr = 0.001, \n",
            "batch no = 2088 / 2323, train loss = 4.456, lr = 0.001, \n",
            "batch no = 2320 / 2323, train loss = 4.376, lr = 0.001, \n",
            "Validation set perplexity : 99.562\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.885\n",
            "Training over\n",
            "Epoch 48/50\n",
            "batch no = 0 / 2323, train loss = 5.150, lr = 0.001, \n",
            "batch no = 232 / 2323, train loss = 4.236, lr = 0.001, \n",
            "batch no = 464 / 2323, train loss = 4.321, lr = 0.001, \n",
            "batch no = 696 / 2323, train loss = 4.255, lr = 0.001, \n",
            "batch no = 928 / 2323, train loss = 4.163, lr = 0.001, \n",
            "batch no = 1160 / 2323, train loss = 4.400, lr = 0.001, \n",
            "batch no = 1392 / 2323, train loss = 4.462, lr = 0.001, \n",
            "batch no = 1624 / 2323, train loss = 4.250, lr = 0.001, \n",
            "batch no = 1856 / 2323, train loss = 4.111, lr = 0.001, \n",
            "batch no = 2088 / 2323, train loss = 4.355, lr = 0.001, \n",
            "batch no = 2320 / 2323, train loss = 4.432, lr = 0.001, \n",
            "Validation set perplexity : 99.558\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.882\n",
            "Training over\n",
            "Epoch 49/50\n",
            "batch no = 0 / 2323, train loss = 5.126, lr = 0.001, \n",
            "batch no = 232 / 2323, train loss = 4.182, lr = 0.001, \n",
            "batch no = 464 / 2323, train loss = 4.370, lr = 0.001, \n",
            "batch no = 696 / 2323, train loss = 4.261, lr = 0.001, \n",
            "batch no = 928 / 2323, train loss = 4.127, lr = 0.001, \n",
            "batch no = 1160 / 2323, train loss = 4.450, lr = 0.001, \n",
            "batch no = 1392 / 2323, train loss = 4.456, lr = 0.001, \n",
            "batch no = 1624 / 2323, train loss = 4.227, lr = 0.001, \n",
            "batch no = 1856 / 2323, train loss = 3.963, lr = 0.001, \n",
            "batch no = 2088 / 2323, train loss = 4.428, lr = 0.001, \n",
            "batch no = 2320 / 2323, train loss = 4.306, lr = 0.001, \n",
            "Validation set perplexity : 99.555\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.882\n",
            "Training over\n",
            "Epoch 50/50\n",
            "batch no = 0 / 2323, train loss = 5.103, lr = 0.001, \n",
            "batch no = 232 / 2323, train loss = 4.182, lr = 0.001, \n",
            "batch no = 464 / 2323, train loss = 4.337, lr = 0.001, \n",
            "batch no = 696 / 2323, train loss = 4.250, lr = 0.001, \n",
            "batch no = 928 / 2323, train loss = 4.143, lr = 0.001, \n",
            "batch no = 1160 / 2323, train loss = 4.478, lr = 0.001, \n",
            "batch no = 1392 / 2323, train loss = 4.435, lr = 0.001, \n",
            "batch no = 1624 / 2323, train loss = 4.306, lr = 0.001, \n",
            "batch no = 1856 / 2323, train loss = 4.032, lr = 0.001, \n",
            "batch no = 2088 / 2323, train loss = 4.462, lr = 0.001, \n",
            "batch no = 2320 / 2323, train loss = 4.370, lr = 0.001, \n",
            "Validation set perplexity : 99.562\n",
            "*************************************************\n",
            "\n",
            "Test set perplexity : 95.882\n",
            "Training over\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot perplexity vs epoch of test and train data from json files\n",
        "summary = {'LSTM Model' : [],\n",
        "           'LSTM Dropout Model': [],\n",
        "           'GRU Model': [],\n",
        "           'GRU Dropout Model':[]}\n",
        "with open('LSTM.json', mode='r', encoding='utf-8') as json_f:\n",
        "    results_dict = json.load(json_f)\n",
        "accuracies_regular_model = plt.figure()\n",
        "plt.plot(range(1, len(results_dict['train_perp']) + 1),\n",
        "             results_dict['train_perp'])\n",
        "plt.plot(range(1, len(results_dict['test_perp']) + 1),\n",
        "             results_dict['test_perp'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('Perplexity vs epoch for LSTM model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('perplexity')\n",
        "accuracies_regular_model.savefig('LSTM_perplexity_plot.png')\n",
        "plt.show()\n",
        "summary['LSTM Model'].append(min(results_dict['train_perp']))\n",
        "summary['LSTM Model'].append(min(results_dict['test_perp']))\n",
        "summary['LSTM Model'].append(min(results_dict['val_perp']))\n",
        "\n",
        "with open('LSTM_dropout.json', mode='r', encoding='utf-8') as json_f:\n",
        "    results_dict = json.load(json_f)\n",
        "accuracies_dropout_model = plt.figure()\n",
        "plt.plot(range(1, len(results_dict['train_perp']) + 1),\n",
        "             results_dict['train_perp'])\n",
        "plt.plot(range(1, len(results_dict['test_perp']) + 1),\n",
        "             results_dict['test_perp'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('Perplexity vs epoch for LSTM dropout model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('perplexity')\n",
        "accuracies_dropout_model.savefig('LSTM_dropout_perplexity_plot.png')\n",
        "plt.show()\n",
        "summary['LSTM Dropout Model'].append(min(results_dict['train_perp']))\n",
        "summary['LSTM Dropout Model'].append(min(results_dict['test_perp']))\n",
        "summary['LSTM Dropout Model'].append(min(results_dict['val_perp']))\n",
        "\n",
        "with open('GRU.json', mode='r', encoding='utf-8') as json_f:\n",
        "    results_dict = json.load(json_f)\n",
        "accuracies_wd_model = plt.figure()\n",
        "plt.plot(range(1, len(results_dict['train_perp']) + 1),\n",
        "             results_dict['train_perp'])\n",
        "plt.plot(range(1, len(results_dict['test_perp']) + 1),\n",
        "             results_dict['test_perp'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('Perplexity vs epoch for GRU model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('perplexity')\n",
        "accuracies_wd_model.savefig('GRU_perplexity_plot.png')\n",
        "plt.show()\n",
        "summary['GRU Model'].append(min(results_dict['train_perp']))\n",
        "summary['GRU Model'].append(min(results_dict['test_perp']))\n",
        "summary['GRU Model'].append(min(results_dict['val_perp']))\n",
        "\n",
        "with open('GRU_dropout.json', mode='r', encoding='utf-8') as json_f:\n",
        "    results_dict = json.load(json_f)\n",
        "accuracies_bn_model = plt.figure()\n",
        "plt.plot(range(1, len(results_dict['train_perp']) + 1),\n",
        "             results_dict['train_perp'])\n",
        "plt.plot(range(1, len(results_dict['test_perp']) + 1),\n",
        "             results_dict['test_perp'])\n",
        "plt.legend(['train','test'])\n",
        "plt.title('Perplexity vs epoch for GRU dropout model')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('perplexity')\n",
        "accuracies_bn_model.savefig('GRU_dropout_perplexity_plot.png')\n",
        "plt.show()\n",
        "summary['GRU Dropout Model'].append(min(results_dict['train_perp']))\n",
        "summary['GRU Dropout Model'].append(min(results_dict['test_perp']))\n",
        "summary['GRU Dropout Model'].append(min(results_dict['val_perp']))"
      ],
      "metadata": {
        "id": "WDASKjPlp-4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"{:<20} {:<20} {:<20} {:<20}\".format('MODEL', 'VALIDATION', 'TRAIN', 'TEST'))\n",
        " \n",
        "#print max train and test accuracy for each model, summarize in a table\n",
        "for key, value in summary.items():\n",
        "    train,test,val = value\n",
        "    print(\"{:<20} {:<20} {:<20} {:<20}\".format(key, val, train, test))"
      ],
      "metadata": {
        "id": "yz9P4BOkr_Qj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}